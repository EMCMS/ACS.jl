{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b96d6c",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "Singular Value Decomposition (SVD) is a powerful linear algebra technique that decomposes any real-valued matrix  *$X_{m \\times n}$*  into three matrices:\n",
    "# \n",
    "$\n",
    " X = U D V^T\n",
    "$\n",
    "\n",
    "## where:\n",
    "- *$U$*: Left singular vectors (matrix of size m $\\times$ n)\n",
    "- *D*: Diagonal matrix of singular values (size n $\\times$ n)\n",
    "- *$V^T$*: Right singular vectors (matrix of size n $\\times$ n)\n",
    "# \n",
    "## SVD is useful for:\n",
    "- **Clustering**\n",
    "- **Dimensionality Reduction**\n",
    "- **Noise Reduction**\n",
    "- **Data Compression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52889041",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Tip:</b> Please note that the method above and the dimension of the matrices associated with the SVD economy method or reduced SVD, where the data points with zero singular values are removed. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b955bf4",
   "metadata": {},
   "source": [
    "# How"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587f300",
   "metadata": {},
   "source": [
    "Each matrix in SVD represents a specific action. The *U* and *$V^T$* represent a rotation action while the *D* represents a Stretching. The rotation process is facilitated via the special property of these two matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31692440",
   "metadata": {},
   "source": [
    "## E1: \n",
    "\n",
    "What is the most important property of these matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6e8d8",
   "metadata": {},
   "source": [
    "> **Answer to E1:** \n",
    ">\n",
    "> They both are [unitary matrices](https://en.wikipedia.org/wiki/Unitary_matrix).\n",
    ">\n",
    "> $U^TU$ = $UU^T$ = I \n",
    "> \n",
    "> $V^TV$ = $VV^T$ = I  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8df10c",
   "metadata": {},
   "source": [
    "## E2: \n",
    "\n",
    "What is the minimum requirement for a matrix to be invertible? Please try to calculate the inverse of the below matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ACS \n",
    "\n",
    "X = [5 -1 1; 5 7 10]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640841f",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> \n",
    "> You will need a square matrix to be able to calculate its inverse. Please note that if you use *pinv(-)* rather than *inv(-)* the algorithm is actually calculating the inverse of *$inv(X^TX) \\times X^T$* and not inv(*X*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinv(X) .- inv(X'*X)*X'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f90c0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You will need to be able to calculate the inverse of a matrix in order to be able to calculate its eigenvalues values and eigenvectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b27d93",
   "metadata": {},
   "source": [
    "The SVD problem can be divided into two simple equations: \n",
    "\n",
    "- $$X^T X = VD^TDV^T$$ \n",
    "- $$XV = UD $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1ebc2",
   "metadata": {},
   "source": [
    "## E3: \n",
    "\n",
    "Let's calculate the $X^TX$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7098278",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX = X'*X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8122e0",
   "metadata": {},
   "source": [
    "For the second half the first equation we need to calculate the eigenvalues values and eigenvectors of the $X^TX$: \n",
    "\n",
    "$$D = \\sqrt{eigenvalues(X^TX)} $$ \n",
    "\n",
    "$$V = eigenvector(X^TX) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450d73f",
   "metadata": {},
   "source": [
    "## E4: \n",
    "\n",
    "Let's calculate the *D* and *V* for the matrix *X*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = sqrt.(eigvals(XTX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b69b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = eigvecs(XTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646a3ab",
   "metadata": {},
   "source": [
    "## E5: \n",
    "\n",
    "How can we calculate U?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760343d7",
   "metadata": {},
   "source": [
    "> Answer to E5:\n",
    ">\n",
    "> $$U = XVD^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = X * V * inv(diagm(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaec157",
   "metadata": {},
   "source": [
    "## E6: \n",
    "\n",
    "Let's reconstruct the matrix *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = U * diagm(D) * V'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83698255",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Proof:</b> As you can see the matrices X and X_hat are exactly the same, proving that a matrix X with real numbers can be decomposed into three distinct matrices..\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb6c8d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Tip:</b> Please note that the method provided above is adequate for very small matrices. This solution is unique, even though expensive. For larger systems there are different approximation algorithms to decompose matrices of interest.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a271cd",
   "metadata": {},
   "source": [
    "There is an efficient and accurate algorithms built in julia language for doing such calculations quickly, such as the function [svd(-)](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd) within the LinearAlgebra package available through ACS.jl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9830cce",
   "metadata": {},
   "source": [
    "## E7: \n",
    "\n",
    "Can you compute the SVD of the matrix *X* using the built in function svd(-)? What is in the output and are there any differences in the results? If yes, are they important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ba1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8618da5",
   "metadata": {},
   "source": [
    "> Answer to E7: \n",
    ">\n",
    "> You get the matrices *U* and *$V^{T}$* as well as the vector of singular values. The vector os singular values is sorted from the largest to the smallest, which was not the case for manual calculation of SVD. Also the sign of a few values are inverted. However, these inversions do not impact the results at all. They are caused by the algorithm behind the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650a7c7",
   "metadata": {},
   "source": [
    "## E8: \n",
    "\n",
    "Let's try to do the same thing for a larger simulated matrix, for example a matrix of random numbers of 50 by 20. \n",
    "\n",
    "```julia \n",
    "A = -10 .+ (30 .* rand(50, 20))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = -10 .+ (30 .* rand(50, 20))\n",
    "out1 = svd(A)\n",
    "out1.S "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a20dde",
   "metadata": {},
   "source": [
    "## E9: \n",
    "\n",
    "How do you calculate the variance explained by each singular value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = out1.S ./ sum(out1.S)\n",
    "var_exp_cum = cumsum(var_exp)\n",
    "\n",
    "scatter(1:20, var_exp, label=\"Variance explained\")\n",
    "plot!(1:20, var_exp_cum, label=\"Cumulative variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f877207",
   "metadata": {},
   "source": [
    "# SVD in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340c2d2",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "As mentioned before SVD can be used for clustering of different measurements similar to [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). Please note that some of the existing PCA algorithms use SVD as their kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693467f",
   "metadata": {},
   "source": [
    "## E10 \n",
    "\n",
    "Let's perform SVD on the Iris dataset. \n",
    "\n",
    "```julia \n",
    "\n",
    "data = dataset(\"datasets\", \"iris\")\n",
    "describe(data) # Summarizes the dataset\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc527f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the data to a matrix\n",
    "\n",
    "data = dataset(\"datasets\", \"iris\")\n",
    "Y = data[!,\"Species\"]\n",
    "X = Matrix(data[:,1:4]); # The first four columns are selected for this example\n",
    "m = svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce2aef",
   "metadata": {},
   "source": [
    "## E11\n",
    "\n",
    "Are there clear clusters visible in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f00ab",
   "metadata": {},
   "source": [
    "> Answer to E11: \n",
    ">\n",
    "> We can plot the *U[:,1]* vs *U[:,2]* while coloring the dots based on the plant species. We can see that the first two of columns of the *U* represent more than 95$\\%$ of the variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55938f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = m.S ./ sum(m.S) # diag() selects the diagonal values in a matrix\n",
    "scatter(m.U[:,1],m.U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = data[!,\"Species\"])\n",
    "p1 = round(100*var_exp[1],digits=0)\n",
    "p2 = round(100*var_exp[2],digits=0)\n",
    "xlabel!(\"First Singular value ($p1%)\")\n",
    "ylabel!(\"Second Singular value ($p2%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bd077",
   "metadata": {},
   "source": [
    "## E12:\n",
    "\n",
    "Can we define separation boundaries of the three clusters in the *U1* and *U2* space? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4e913",
   "metadata": {},
   "source": [
    "> Answer to E12 \n",
    ">\n",
    "> For Setosa *U2* values larger than 0.05 may be a separation point. On the other hand between the other two types the separation would be for *U2* values $\\leq$ -0.04.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(m.U[:,1],m.U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = data[!,\"Species\"])\n",
    "plot!([-0.15,0],[0.05,0.05],label=\"Setosa\")\n",
    "plot!([-0.15,0],[-0.04,-0.04],label=\"Virginica\")\n",
    "xlabel!(\"First Singular value ($p1%)\")\n",
    "ylabel!(\"Second Singular value ($p2%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf85aa8",
   "metadata": {},
   "source": [
    "## E13: \n",
    "\n",
    "Can we randomly select 10 cases and predict the flower type based on the four variables of and our SVD model?\n",
    "\n",
    "```julia \n",
    "\n",
    "n = 10 # number of points to be selected\n",
    "\n",
    "rand_ind = rand(1:size(X,1),n) # generate a set of random numbers between 1 and size(X,1)\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ad1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of points to be selected\n",
    "\n",
    "rand_ind = rand(1:size(X,1),n) # generate a set of random numbers between 1 and size(X,1)\n",
    "X_1 = X[rand_ind,:] # select the rows of X corresponding to the random indices\n",
    "# U = X * V * inv(diagm(D))\n",
    "U_ = X_1 * m.V * inv(diagm(m.S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(m.U[:,1],m.U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = data[!,\"Species\"])\n",
    "plot!([-0.15,0],[0.05,0.05],label=\"Setosa\")\n",
    "plot!([-0.15,0],[-0.04,-0.04],label=\"Virginica\")\n",
    "xlabel!(\"First Singular value ($p1%)\")\n",
    "ylabel!(\"Second Singular value ($p2%)\")\n",
    "scatter!(U_[:,1],U_[:,2],label=\"Random points\",group =data[rand_ind,\"Species\"],marker=:star,ms=10,alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26542651",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Use blue boxes (alert-info) for tips and notes. \n",
    "As you can see in the plot above, as expected, the randomly selected datasets land exactly on top of the original data, indicating a correct prediction. It should be noted that normally, not all the data is used for the model building. A portion of data is usually left for testing and validation while building the model..\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95538907",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "SVD can also be used for the dimension reduction of large datasets. The dimension reduction either helps removing certain variables or generates a set of new variables that are linear combination of the original ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd62c5",
   "metadata": {},
   "source": [
    "## E14: \n",
    "\n",
    "What are the variables with the highest level of information to form the above clusters for the Iris data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfcd6d",
   "metadata": {},
   "source": [
    "> Answer to E14\n",
    ">\n",
    "> We need to look at the rows of *Vt*. Each row there represents \"loadings\"/weights associated with each variable and/or singular value. For example for the first SV all the loadings are negative, indicating a negative relationship between the model projection (i.e. *U*) and all the four variables. The magnitude of each loading is indicative of the variable importance. In this case for *U1* variables one and three are the most important ones while for the *U2* variables three and two are the most relevant ones. \n",
    ">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf049b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(m.Vt[1,:],label=\"First SV\")\n",
    "bar!(m.Vt[2,:],label=\"Second SV\",alpha = 0.5)\n",
    "xlabel!(\"Variable Nr\")\n",
    "ylabel!(\"Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = scatter(X[:,1],m.U[:,1],label=false)\n",
    "xlabel!(\"SepalLength\")\n",
    "ylabel!(\"Scores U1\")\n",
    "\n",
    " p2 = scatter(X[:,4],m.U[:,1],label=false)\n",
    "xlabel!(\"PetalLength\")\n",
    "ylabel!(\"Scores U1\")\n",
    "\n",
    " p3 = scatter(X[:,2],m.U[:,2],label=false)\n",
    "xlabel!(\"SepalWidth\")\n",
    "ylabel!(\"Scores U2\")\n",
    "\n",
    "p4 = scatter(X[:,3],m.U[:,2],label=false)\n",
    "xlabel!(\"PetalLength\")\n",
    "ylabel!(\"Scores U2\")\n",
    "\n",
    "plot(p1,p2,p3,p4,layout = (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b063b5c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Proof:</b> As you can see in the below figure using the two variables with the highest loadings values. We were able to identify these taking advantage of the loading plots. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba968be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X[:,1],X[:,3],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = data[!,\"Species\"])\n",
    "xlabel!(\"SepalLength\")\n",
    "ylabel!(\"PetalLength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8f4f9",
   "metadata": {},
   "source": [
    "## E15:\n",
    "\n",
    "The dimension reduction is particularly helpful for solving regression problems where there are many variables. Let's imagine a case where we have 100 measurements and 50 variables. We would like to use the minimum number of variables to predict the target *Y*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "# Generate synthetic dataset (100 samples, 50 features)\n",
    "n_samples = 100\n",
    "n_features = 50\n",
    "# Create correlated feature matrix (random linear combinations)\n",
    "X = randn(n_samples, n_features) * randn(n_features, n_features)  # Introduce correlation\n",
    "# Generate target variable with some noise\n",
    "b = randn(n_features)  # True coefficients\n",
    "y = X * b + 0.01 * randn(n_samples);  # Linear relationship with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Singular Value Decomposition (SVD)\n",
    "U, S, Vt = svd(X)\n",
    "\n",
    "# compute variance explained\n",
    "var_exp = S ./ sum(S)\n",
    "var_exp_cum = cumsum(var_exp)\n",
    "scatter(1:n_features, var_exp, label=\"Variance explained\")\n",
    "plot!(1:n_features, var_exp_cum, label=\"Cumulative variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e24ff",
   "metadata": {},
   "source": [
    "Given that we want to have minimum number of variables, we can choose 12 SV representing around 50% of variance in the data. At this point we treat this as a simple least square problem where the first 12 columns of *U* are our *X* and *Y* remains as it was. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbdbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_h = pinv(transpose(U[:,1:12]) * U[:,1:12]) * transpose(U[:,1:12]) * y # simple least square solution\n",
    "y_hat = U[:,1:12] * b_h # prediction the y_hat\n",
    "\n",
    "scatter(y,y_hat)\n",
    "plot!([minimum(y),maximum(y)],[minimum(y),maximum(y)],label=\"y = y_hat\",color=:red)\n",
    "xlabel!(\"True y\")\n",
    "ylabel!(\"Predicted y\")  # The first 12 columns are selected for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c78c6",
   "metadata": {},
   "source": [
    "In this example with around 20% of the variables we are able to reasonably predict *Y*, showing the power of SVD in capturing the underlying trends in a dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cff83c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Use blue boxes (alert-info) for tips and notes. \n",
    "Each SV in the SVD, represent a very specific underlying trend in the data. These trends may not be visible when looking at the actual variables. For example in case of Iris dataset the two variables SepalLength vs SepalWidth have a one to two relationship, which is very difficult to observe. SVD is able to capture this trend in the first SV.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_temp = m.S\n",
    "D_temp[2:end] .= 0\n",
    "D_n = diagm(D_temp) # the new singular value matrix\n",
    "X_h  = m.U * D_n * m.Vt # the reconstructed matrix\n",
    "scatter(data[:,1],data[:,2],label=\"X\")\n",
    "scatter!(X_h[:,1],X_h[:,2],label=\"X_h\")\n",
    "xlabel!(\"SepalLength\")\n",
    "ylabel!(\"SepalWidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b154f9",
   "metadata": {},
   "source": [
    "## E16: \n",
    "\n",
    "Load the mtcars dataset and perform SVD on that. What are the main underlying trends in the data. Compare the results of the analysis with and without mean centering and scaling\n",
    "\n",
    "```julia \n",
    "\n",
    "data = read_ACS_data(“mtcars.csv”)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
