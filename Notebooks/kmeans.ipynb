{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6a57d",
   "metadata": {},
   "source": [
    "# Hierarchical Cluster Analysis (HCA)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hierarchical Cluster Analysis (HCA) is a method for grouping similar objects into clusters based on their similarity. Unlike flat clustering methods like k-means, HCA creates a hierarchy of nested clusters that can be visualized using a dendrogram.\n",
    "\n",
    "There are two main types of hierarchical clustering:\n",
    "\n",
    "1. **Agglomerative (bottom-up)**: Each data point starts as its own cluster, and clusters are merged iteratively.\n",
    "2. **Divisive (top-down)**: All data points start in one cluster, and clusters are split iteratively.\n",
    "\n",
    "### Applications of HCA\n",
    "HCA is widely used in various fields manily for trend analysis, including:\n",
    "- **Bioinformatics & Genomics**: Identifying gene expression patterns and evolutionary relationships.\n",
    "- **Market Segmentation**: Grouping customers based on purchasing behavior.\n",
    "- **Image Processing**: Clustering pixels with similar attributes for object recognition.\n",
    "- **Environmental Science**: Categorizing chemical pollutants based on their similarities.\n",
    "- **Social Sciences**: Analyzing survey responses to detect behavioral patterns.\n",
    "\n",
    "Here we are going to focuse on the agglomerative approach as it is the most commonly used approach for HCA implementation. \n",
    "\n",
    "### Steps of HCA\n",
    "1. Compute the pairwise distances between all points.\n",
    "2. Identify the two closest clusters and merge them.\n",
    "3. Update the distance matrix.\n",
    "4. Repeat steps 2 and 3 until all points are in one cluster.\n",
    "5. Represent the results using a **dendrogram**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba08f1a",
   "metadata": {},
   "source": [
    "## How \n",
    "\n",
    "To be able to go through different steps of HCA algorithm we first need to generate some synthetic data that we can use. Please note that the generated dataset is one dimensional for ease of explanation and visualization. The same approach can be expanded to multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ACS\n",
    "\n",
    "ACS.Random.seed!(42)\n",
    "X = vcat(rand(5, 2) .+ 1, rand(5, 2) .+ 4)  # Two distinct clusters\n",
    "scatter(X[:,1],X[:,2],label=\"data\")\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23cada",
   "metadata": {},
   "source": [
    "Let's ignore the second dimension of *X* for the moment and only focus on the 1st dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[:,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13851069",
   "metadata": {},
   "source": [
    "As we are using working with the agglomerative approach, we start with assuming that all the data points are individual clusters. So in this case we have 10 starting clusters. \n",
    "\n",
    "### Step 1: pair-wise distance calculations \n",
    "\n",
    "In unidimensional space the distance between two datapoints are defined by the below equation: \n",
    "\n",
    "$ \n",
    "d_{m,n} = x_n - x_m \\\\\n",
    "m,n \\in \\mathbb{N}^+ \\\\\n",
    "n \\geq m\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da878eb",
   "metadata": {},
   "source": [
    "### E1: \n",
    "\n",
    "Let's write a function to calculate 1D distances between all the points in the *X1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30757f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function dist(data)\n",
    "    d = zeros(size(data,1),size(data,1)) #initialize distance matrix\n",
    "    for i in 1:size(data,1) #loop over all data points\n",
    "        for j in 1:size(data,1) #loop over all data points\n",
    "            d[i,j] = abs(data[i]-data[j]) #calculate absolute distance\n",
    "        end\n",
    "    end\n",
    "    return d    \n",
    "\n",
    "end \n",
    "\n",
    "d = dist(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149436a",
   "metadata": {},
   "source": [
    "### E2: \n",
    "\n",
    "Why do we get a square matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14584e7a",
   "metadata": {},
   "source": [
    "> Answer to E2\n",
    "> \n",
    "> The square matrix is the result of pair-wise distance calculations as you calculate the distances for all possible combinations of indices. The diagonal is zero as the distance of each point with itslef is zero. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4ac8b",
   "metadata": {},
   "source": [
    "### E3: \n",
    "\n",
    "How do we form the first cluster? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee52a5",
   "metadata": {},
   "source": [
    "> Answer to E3:\n",
    ">\n",
    "> The smallest distances that is not on the diagonal gives you the coordinates of the first cluster. To do these calculations, you can first replace the diagonal with *Inf*, enabling you to search for the smallest non-zero distances. We can see that the fourth and first data-points can form the first cluster. \n",
    ">\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = deepcopy(d)\n",
    "d_[d_ .== 0] .= Inf\n",
    "d_\n",
    "\n",
    "idx = argmin(d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(X1, X[:,2], label=\"data\")\n",
    "scatter!([X1[idx[1]] X1[idx[2]]], [X[:,2][idx[1]] X[:,2][idx[2]]], label=\"closest pair\", color=\"red\")\n",
    "#xlims!(1, 1.5)\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9dada",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Tip:</b> It is very important to use deepcopy(-) to make sure that the original distance matrix (d) is not modified during the agglomerative clustering.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c0906",
   "metadata": {},
   "source": [
    "### E4:\n",
    "\n",
    "What is the next step? How can we move forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee0c0c",
   "metadata": {},
   "source": [
    "> Answer to E4:\n",
    ">\n",
    "> We replace the data points clustered together with their average, which should sit in between them. \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = deepcopy(X)\n",
    "X_c[idx[1],:] = (X[idx[1],:] .+ X[idx[2],:])./2\n",
    "X_c[idx[2],:] = (X[idx[1],:] .+ X[idx[2],:])./2\n",
    "\n",
    "scatter(X[:,1],X[:,2],label=\"original data\")\n",
    "scatter!(X_c[:,1],X_c[:,2],label=\"new data\",color=\"red\")\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbeb5c2",
   "metadata": {},
   "source": [
    "### E5: \n",
    "\n",
    "Let's find the next cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6adbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dist(X_c[:,1])\n",
    "d[d .== 0] .= Inf\n",
    "idx = argmin(d)\n",
    "scatter(X_c[:,1], X_c[:,2], label=\"data 2nd itr\")\n",
    "scatter!([X_c[:,1][idx[1]] X_c[:,1][idx[2]]], [X_c[:,2][idx[1]] X_c[:,2][idx[2]]], label=\"closest pair\", color=\"red\")\n",
    "#xlims!(1, 1.5)\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec60469",
   "metadata": {},
   "source": [
    "If you repeat this process several times you will eventually end up with all the data points being agglomerated into one single large cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebb47a",
   "metadata": {},
   "source": [
    "### E6: \n",
    "\n",
    "How do you know that all the data points have been clustered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c6de9",
   "metadata": {},
   "source": [
    "> Answer to E6\n",
    ">\n",
    "> Over each iteration the data points clustered together have a zero distance. Consequently, once all data points are clustered into one, all distances will be zero. \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3874d",
   "metadata": {},
   "source": [
    "### E7: \n",
    "\n",
    "So far we only looked at a one dimensional dataset to be clustered. What if we want to do this with a multi-dimensional data (e.g. *X* rather than *X1*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3cc399",
   "metadata": {},
   "source": [
    "> Answer to E7: \n",
    ">\n",
    "> We need to modify the distance calculation function to allow more than one dimension. Please note for the time being we will only focus on the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). There are many other distance metrics that will be discussed later on. \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9135f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function dist_euc(data)\n",
    "    d = zeros(size(data,1),size(data,1)) #initialize distance matrix\n",
    "    for i in 1:size(data,1) #loop over all data points\n",
    "        for j in 1:size(data,1) #loop over all data points\n",
    "            d[i,j] = sqrt(sum((data[i,:] .- data[j,:]).^2)) #calculate Euclidean distance\n",
    "        end\n",
    "    end\n",
    "    return d    \n",
    "\n",
    "end \n",
    "\n",
    "d = dist_euc(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58389775",
   "metadata": {},
   "source": [
    "### E8: \n",
    "\n",
    "What are the first data points to cluster together with the new distance matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042184d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = deepcopy(d)\n",
    "d_[d_ .== 0] .= Inf\n",
    "idx = argmin(d_)\n",
    "\n",
    "scatter(X[:,1], X[:,2], label=\"data\")\n",
    "scatter!([X[:,1][idx[1]] X[:,1][idx[2]]], [X[:,2][idx[1]] X[:,2][idx[2]]], label=\"closest pair\", color=\"red\")\n",
    "#xlims!(1, 1.5)\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d37f8",
   "metadata": {},
   "source": [
    "As you can see in the above plot, depending on the number of dimensions used the starting cluster may be different. \n",
    "\n",
    "The package [*Clustering.jl*](https://juliastats.org/Clustering.jl/stable/hclust.html), available via *ACS.jl* gives to access to a set of functions that are very useful for your cluster analysis. \n",
    "\n",
    "## HCA in practice: \n",
    "\n",
    "The main function to be used for HCA is [*hclust(-)*](https://juliastats.org/Clustering.jl/stable/hclust.html#Clustering.hclust). This function outputs the generated clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735e88c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Use blue boxes (alert-info) for tips and notes. \n",
    "It should be noted that the function hclust takes as input your distance matrix. Thus all clusters start with Euclidean distance matrix. However, you can select a different distance metrics for grouping on two clusters. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fce96",
   "metadata": {},
   "source": [
    "### E9: \n",
    "\n",
    "Let's perform HCA on the matrix X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dist_euc(X) # Euclidean distance matrix\n",
    "result = hclust(d, linkage=:single) # Single linkage clustering\n",
    "plot(result, labels=1:size(X,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Single Linkage Clustering\") # Plot the dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061153bf",
   "metadata": {},
   "source": [
    "The results of HCA is usually displayed via [dendrogram](https://en.wikipedia.org/wiki/Dendrogram), figure above. Sometimes a combination of dendrogram and heatmaps are used for analyzing the results of HCA. The output of HCA has two attributes \"order\" and \"merges\". The attribute \"order\" gives you the order of the clusters from left to right - in example above from 3 to 10. The \"merges\" shows what measurements were merged and at what level. \n",
    "\n",
    "```julia \n",
    "\n",
    "result.order # Clusters\n",
    "result.merges # Merges\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X[result.order,:] # Reorder the data matrix\n",
    "p1 = heatmap(X_, color=:jet, aspect_ratio=1, title=\"Single Linkage Clustering\",cbar= :none, yaxis = false) # Plot the heatmap\n",
    "xlims!(0, 3) # Set the x-axis limits\n",
    "p2 = plot(result, labels=1:size(X,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :horizontal, linewidth=1, title=\"Single Linkage Clustering\") # Plot the dendrogram\n",
    "plot(p1, p2, layout=(1,2), size=(800,400)) # Combine the heatmap and dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b631e",
   "metadata": {},
   "source": [
    "You can also cut the dendrogram to generate a specific number of clusters. This is done via the function *cutree()* via *ACS.jl/Clustering.jl*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ba1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_c = cutree(result, k=2) # Cut the dendrogram into 2 clusters\n",
    "scatter(X[:,1], X[:,2], label=[\"Cluster 1\" \"Cluster 2\"], group = idx_c, color=idx_c, legend=:bottomright, title=\"Single Linkage Clustering\") # Plot the data points with cluster labels\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_c = cutree(result, k=3) # Cut the dendrogram into 2 clusters\n",
    "scatter(X[:,1], X[:,2], label=[\"Cluster 1\" \"Cluster 2\" \"Cluster 3\"], group = idx_c, color=idx_c, legend=:bottomright, title=\"Single Linkage Clustering\") # Plot the data points with cluster labels\n",
    "xlabel!(\"1st Dim\")\n",
    "ylabel!(\"2nd Dim\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c73316",
   "metadata": {},
   "source": [
    "### E10: \n",
    "\n",
    "How can we use another distance metrics, for example Jaccard? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722dc9c",
   "metadata": {},
   "source": [
    "> Answer to 10\n",
    ">\n",
    "> You can use the function *pairwise(-)* from *ACS.jl/Distances.jl* with the keyword Mahalanobis to perform these calculations.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2889452",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pairwise(Jaccard(), X,dims = 1) # Mahalanobis distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70185e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_c = hclust(d, linkage=:single) # Single linkage clustering\n",
    "plot(r_c, labels=1:size(X,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Single Linkage Clustering\") # Plot the dendrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5127383",
   "metadata": {},
   "source": [
    "As you can see the composition of the two large clusters did not change while the immediate connections in some cases are slightly different from the previously generated clusters based on Euclidean distances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbc701",
   "metadata": {},
   "source": [
    "### E11:\n",
    "\n",
    "Let's generate a more complex dataset and assess the impact of different linkages on the final clusters. The available linkage methods are the following:\n",
    "\n",
    "- **:single**: distance between the closest points of two clusters.\n",
    "- **:average**: use the mean distance between any of the cluster members \n",
    "- **:complete**: distance between the farthest points of two clusters.\n",
    "- **:ward**: the distance is the increase of the average squared distance of a point to its cluster centroid after merging the two clusters.\n",
    "\n",
    "```julia\n",
    "\n",
    "Z = rand(15, 2) * 10 # data generation\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0812437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rand(15, 2) * 10\n",
    "d = pairwise(Euclidean(), Z,dims = 1) # Euclidean distance matrix\n",
    "r_c_s = hclust(d, linkage=:single) # Single linkage clustering\n",
    "r_c_a = hclust(d, linkage=:average) # Average linkage clustering\n",
    "r_c_c = hclust(d, linkage=:complete) # Complete linkage clustering\n",
    "r_c_w = hclust(d, linkage=:ward) # Ward linkage clustering\n",
    "\n",
    "p1 = plot(r_c_s, labels=1:size(Z,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Single Linkage Clustering\") # Plot the dendrogram\n",
    "p2 = plot(r_c_a, labels=1:size(Z,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Average Linkage Clustering\") # Plot the dendrogram\n",
    "p3 = plot(r_c_c, labels=1:size(Z,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Complete Linkage Clustering\") # Plot the dendrogram\n",
    "p4 = plot(r_c_w, labels=1:size(Z,1), color_threshold=0.5, show_leaf_labels=true,\n",
    " show_inner_nodes=true, show_root=true, linecolor=:black, orientation = :v, linewidth=1, title=\"Ward Linkage Clustering\") # Plot the dendrogram\n",
    "plot(p1, p2, p3, p4, layout=(2,2), size=(800,800)) # Combine the dendrograms    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc63016",
   "metadata": {},
   "source": [
    "As you can see, changing the merging metrics could drastically change the structure of the generated clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80441e98",
   "metadata": {},
   "source": [
    "### E12: \n",
    "\n",
    "How do you decide which linkage is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfb531",
   "metadata": {},
   "source": [
    "> Answer to E12:\n",
    ">\n",
    "> Generally for any hyperparameter optimization, you should have some labeled data, enabling you to assess the quality of the generated clusters. This also can be done via expert knowledge. For example, if you a priori know that two samples are supposed to be very similar to each other (e.g. two replicates) the linkage approach that cluster them with no intermediates is more adequate to answer your scientific question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddda6b",
   "metadata": {},
   "source": [
    "## Extensive Exercise: \n",
    "\n",
    "Use HCA for the analysis of Iris dataset. Assess which linkage approach is performing the best and finally plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset(\"datasets\", \"iris\")\n",
    "X_iris = Matrix(data[:, 1:4])  # Selecting only numerical features\n",
    "\n",
    "\"\"\"\n",
    "## 2. Computing Pairwise Distances\n",
    "We compute the pairwise distance matrix using Euclidean distance.\n",
    "\"\"\"\n",
    "\n",
    "dist_iris = pairwise(Euclidean(), X_iris, dims=1)\n",
    "\n",
    "\"\"\"\n",
    "## 3. Performing Hierarchical Clustering using Different Linkage Methods\n",
    "We compare Single, Complete, and Average linkage methods.\n",
    "\"\"\"\n",
    "\n",
    "linkage_single = hclust(dist_iris, linkage=:single)\n",
    "linkage_complete = hclust(dist_iris, linkage=:complete)\n",
    "linkage_average = hclust(dist_iris, linkage=:average)\n",
    "linkage_ward = hclust(dist_iris, linkage=:ward)\n",
    "\n",
    "\"\"\"\n",
    "## 4. Visualizing the Dendrograms\n",
    "We plot the dendrograms for each linkage method.\n",
    "\"\"\"\n",
    "\n",
    "p1 = plot(linkage_single, xticks=false, title=\"HCA with Single Linkage\", xlabel=\"Samples\", ylabel=\"Distance\")\n",
    "p2 = plot(linkage_complete, xticks=false, title=\"HCA with Complete Linkage\", xlabel=\"Samples\", ylabel=\"Distance\")\n",
    "p3 = plot(linkage_average, xticks=false, title=\"HCA with Average Linkage\", xlabel=\"Samples\", ylabel=\"Distance\")\n",
    "p4 = plot(linkage_ward, xticks=false, title=\"HCA with Ward Linkage\", xlabel=\"Samples\", ylabel=\"Distance\")\n",
    "plot(p1, p2, p3, p4, layout=(2, 2), size=(800, 800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 5. Evaluating Clustering Performance\n",
    "To assess the clustering performance, we compare the results to the actual species labels using Adjusted Rand Index (ARI).\n",
    "\"\"\"\n",
    "\n",
    "# Extract cluster assignments\n",
    "clusters_single = cutree(linkage_single, k=3)\n",
    "clusters_complete = cutree(linkage_complete, k=3)\n",
    "clusters_average = cutree(linkage_average, k=3)\n",
    "clusters_ward = cutree(linkage_ward, k=3)\n",
    "\n",
    "# True labels\n",
    "true_labels = [data.Species[i] == \"setosa\" ? 1 : data.Species[i] == \"versicolor\" ? 2 : 3 for i in 1:size(data, 1)]\n",
    "\n",
    "# Compute Adjusted Rand Index (ARI)\n",
    "ari_single = ACS.randindex(clusters_single, true_labels)\n",
    "ari_complete = ACS.randindex(clusters_complete, true_labels)\n",
    "ari_average = ACS.randindex(clusters_average, true_labels)\n",
    "ari_ward = ACS.randindex(clusters_ward, true_labels)\n",
    "\n",
    "println(\"ARI for Single Linkage: \", ari_single)\n",
    "println(\"ARI for Complete Linkage: \", ari_complete)\n",
    "println(\"ARI for Average Linkage: \", ari_average)\n",
    "println(\"ARI for Ward Linkage: \", ari_ward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
