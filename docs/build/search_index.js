var documenterSearchIndex = {"docs":
[{"location":"HCA/#Hierarchical-Cluster-Analysis-(HCA)","page":"HCA","title":"Hierarchical Cluster Analysis (HCA)","text":"","category":"section"},{"location":"HCA/#Introduction","page":"HCA","title":"Introduction","text":"","category":"section"},{"location":"HCA/","page":"HCA","title":"HCA","text":"The HCA is an unsupervised clustering approach mainly based on the distances of the measurements from each other. It is an agglomerative approach, thus starting with each individual measurement as a cluster and then grouping them to build a final cluster that includes all the measurements.   ","category":"page"},{"location":"HCA/#How?","page":"HCA","title":"How?","text":"","category":"section"},{"location":"HCA/","page":"HCA","title":"HCA","text":"The approach taken in HCA is very simple from programming point of view. The algorithm starts with the assumption that every individual measurement is a unique cluster. Then it calculates the pairwise distances between the measurements. The two measurements with the smallest distance are grouped together to form the first agglomerative cluster. In the next iteration, the newly generated cluster is then represented by either its mean, minimum or its maximum for the distance calculations. It should be noted that there are several ways to calculate the distance between two measurements (e.g. Euclidean distance and Mahalanobis distance). For simplicity, we are only going to look at the \"Euclidean distance\" here.   ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"In a one dimensional space, the distance between points x_n and x_m is calculated by subtracting the two points from each other.","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"\nd_mn = x_n - x_m\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"Assuming the below dataset with vectors X and Y as the coordinates. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\n# Generating the data\n\ncx1 = 1 .+ (2-1) .* rand(5) # 5 random values between 1 and 2 \nc1 = 5 .* rand(5)           # 5 random values around 5\ncx2 = 4 .+ (6-4) .* rand(5) # 5 random values between 4 and 6\nc2 = 10 .* rand(5)          # 5 random values around 10\n\nY = vcat(c1[:],c2[:])       # building matrix Y\nX = vcat(cx1[:],cx2[:])     # building the matrix X\n\n# Plotting the data\nscatter(cx1,c1,label = \"Cluster 1\")\nscatter!(cx2,c2,label = \"Cluster 2\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"We first plot the data in the one dimensional data. In other words, we are setting the y values to zero in our data matrix. Below you can see how this is done.","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\n# Plotting the data\n\nscatter(X,zeros(length(X[:])),label = \"Data\")\n\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"The next step is to calculate the distances in the x domain. For that we are using the Euclidean distances. Here we need to calculate the distance between each point in the X and all the other values in the same matrix. This implies that we will end up with a square distance matrix (i.e. dist). The dist matrix has a zero diagonal, given that the values on the diagonal represent the distance between each point and itself. Also it is important to note that we are interested only in the magnitude of the distance but not its direction. Thus, you can use the abs.(-) to convert all the distances to their absolute values. Below you can see an example of a function for these calculations.","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nfunction dist_calc(data)\n\n    dist = zeros(size(data,1),size(data,1))      # A square matrix is initiated \n\tfor i = 1:size(data,1)-1                     # The nested loops create two unaligned vectors by one member\n\t\tfor j = i+1:size(data,1)\n\t\t\tdist[j,i] = data[j,1] - data[i,1]    # The generated vectors are subtracted from each other \n\t\tend\n\tend\n\n\tdist += dist'                                # The upper diagonal is filled \n\treturn abs.(dist)                            # Make sure the order of subtraction does not affect the distances\n\nend \n\ndist = dist_calc(X)\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"In the next step we need to find the points in the X that have the smallest distance and should be grouped together as the first cluster. To do so we need to use the dist matrix. However, as you see in the dist matrix the smallest values are zero and are found in the diagonal. A way to deal with this is to set the diagonal to for example to Inf. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\n#dist = dist_calc(X) \ndist[diagind(dist)] .= Inf                   # Set the diagonal to inf, which is very helpful when searching for minimum distance\ndist\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"Now that we have the complete distances matrix, we can use the function argmin(-) to find the coordinates of the points with the minimum distance in the X. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\ncl_temp = argmin(dist)\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"The selected points in the X are the closest to each other, indicating that they should be grouped into one cluster. In the next step, we will assume this cluster as a single point and thus we can repeat the distance calculations. For simplicity, we assume that the average of the two points is representative of that cluster. This process is called linkage and can be done using different approaches. Using the mean, in particular, is called centroid linkage. With centroid method, we are replacing these points with their average. This process can be repeated until all data points are grouped into one single cluster.","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nX1 = deepcopy(X)\nX1[cl_temp[1]] = mean([X[cl_temp[1]],X[cl_temp[2]]])\nX1[cl_temp[2]] = mean([X[cl_temp[1]],X[cl_temp[2]]])\n\n[X,X1]\n\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\n\nscatter(X,zeros(length(X[:])),label = \"Original data\")\nscatter!(X1,0.01 .* ones(length(X1[:])),label = \"Data after clustering\",fillalpha = 0.5)\nylims!(-0.01,0.1)\nxlabel!(\"X\")\nylabel!(\"Y\")\n\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"So far, we have done all our calculations based on one dimensional data. Now we can move towards two and more dimension. One of the main things to consider when increasing the number of dimensions is the distance calculations. In the above examples, we have use the Euclidean distance, which is one of many distance metrics. In general terms the Euclidean distance can be expressed as below, where d_mn represents the distance between points m and n. This is based on the Pythagorean distance. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"d_mn = sqrtsum(x_m - x_n)^2\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"Let's try to move to a two dimensional space rather than uni-dimensional one. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\n# Plotting the data\nscatter(cx1,c1,label = \"Cluster 1\")\nscatter!(cx2,c2,label = \"Cluster 2\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"The very first step to do so is to convert our 1D distances to 2D ones, using the below equation. If we replace the distji = dataj1 - datai1 with the below equation in our distance function, we will be able to generate the distance matrix for our two dimensional dataset. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"d_mn = sqrt(x_m - x_n)^2 + (y_m - y_n)^2\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nfunction dist_calc(data)\n\n    dist = zeros(size(data,1),size(data,1))      # A square matrix is initiated \n\tfor i = 1:size(data,1)-1                     # The nested loops create two unaligned vectors by one member\n\t\tfor j = i+1:size(data,1)\n\t\t\tdist[j,i] = sqrt(sum((data[i,:] .- data[j,:]).^2))    # The generated vectors are subtracted from each other \n\t\tend\n\tend\n\n\tdist += dist'                                # The upper diagonal is filled \n\treturn abs.(dist)                            # Make sure the order of subtraction does not affect the distances\n\nend \n\ndata = hcat(X,Y)\t\t\t\t\t\t\t\t# To generate the DATA matrix\n\ndist = dist_calc(data) \t\t\t\t\t\t\t# Calculating the distance matrix\ndist[diagind(dist)] .= Inf                   # Set the diagonal to inf, which is very helpful when searching for minimum distance\ndist\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\ncl_temp = argmin(dist)\n\ndata1 = deepcopy(data)\n\ndata1[cl_temp[1],1] = mean([data[cl_temp[1],1],data[cl_temp[2],1]])\ndata1[cl_temp[1],2] = mean([data[cl_temp[1],2],data[cl_temp[2],2]])\ndata1[cl_temp[2],1] = mean([data[cl_temp[1],1],data[cl_temp[2],1]])\ndata1[cl_temp[2],2] = mean([data[cl_temp[1],2],data[cl_temp[2],2]])\n\ndata1\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nscatter(data[:,1],data[:,2],label = \"Original data\")\nscatter!(data1[:,1],data1[:,2],label = \"Data after clustering\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"As it can be seen in the figure, there are two blue points and one red point in the middle of those points. These blue dots represent the two closest data points that are clustered together to form the centroid in between them. If we repeat this process multiple times, we eventually end up having all data points into one large cluster. The HCA clustering generates an array of clustered data points that can be visualized via a dendrogram or a heatmap. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\ndist = dist_calc(data) \n\nhc = hclust(dist, linkage=:average)\nsp.plot(hc)\n","category":"page"},{"location":"HCA/#Practical-Application","page":"HCA","title":"Practical Application","text":"","category":"section"},{"location":"HCA/","page":"HCA","title":"HCA","text":"We can use either our home developed function for HCA or use the julia built-in functions. Here we will provide an easy tutorial on how to use the julia functions that are built-in the ACS.jl package. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"For calculating the distances the function pairwise(-) via the julia package Distances.jl can be used. Function pairwise(-) has three inputs namely: 1) distance metrics, 2) data, and 3) the operation direction. This function outputs a square matrix similar to our distance matrix. As it can be seen from the distance matrix, our function and the pairwise(-) generate the same results, which is expected. The function pairwise(-) will give access to a wide variety of distance metrics that can be used for your projects. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\ndist1 = pairwise(ACS.Euclidean(), data, dims=1) # Euclidean distance \n\n# dist1 = pairwise(ACS.TotalVariation(), data, dims=1) # TotalVariation distance \n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"For the HCA itself, you can use the function hclust(-) incorporated in the ACS.jl package and provided via Clustering.jl package. This function takes two inputs, the distance matrix and the linkage method. The output of this function is a structure with four outputs. The two most important outputs are merges and order. The combination of all four outputs can be plotted via sp.plot(-) function. ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nh = hclust(dist1,:average) # Average linkage or centroids \n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"To access the outputs, one can do the following: ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nh.order \n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"and to plot the outputs, we can use the below function.","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"using ACS\n\nsp.plot(h)\n","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"There are also python implementation of HCA, that you can explore using those for your analysis. ","category":"page"},{"location":"HCA/#Additional-Example","page":"HCA","title":"Additional Example","text":"","category":"section"},{"location":"HCA/","page":"HCA","title":"HCA","text":"If you are interested in practicing more, you can use the mtcars dataset via RDatasets provided in folder dataset of the package ACS.jl github repository. Please note that you must exclude the car origin column. The objective here is to see whether HCA is able to cluster the cars with similar origins.  ","category":"page"},{"location":"HCA/","page":"HCA","title":"HCA","text":"If you are interested in additional resources regarding HCA and would like to know more you can check this MIT course material.  ","category":"page"},{"location":"KMeans/#K-means-Clustering","page":"k-means","title":"K-means Clustering","text":"","category":"section"},{"location":"KMeans/#Introduction","page":"k-means","title":"Introduction","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"The k-means clustering is an unsupervised clustering algorithm that uses the distance metrics to create the user defined number of clusters. The k-means can be used for both continuous and discreet data, given that there are at least two variables provided. The number of clusters to be formed are typically provided by the user and is typically based on the prior knowledge. The center of each cluster in k-means is called centroid and meant to represent that cluster of data. The k-means algorithm is considered a greedy algorithm as at the end every single point in the dataset is part of a specific cluster. Also, k-means is an iterative algorithm , given that the process of finding the local minimum is done over multiple iterations. The algorithmic details of k-means are provided below. ","category":"page"},{"location":"KMeans/#How?","page":"k-means","title":"How?","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"In k-means the algorithm initialized with a set of randomly selected centroids. The number of these centroids is user defined while their location is randomly selected. Based on the Euclidean distances of each point in the dataset and the centroids, the points are assigned to each cluster. Here l defines the number of variables, m is the number of points, c_{n} is the number of the clusters, and x is the coordinates on each point/centroid. It should be noted that m, n, and c all must be real numbers and larger than 0. Additionally, the condition of m > n must be fulfilled. This means that the number of points must be larger than the number of clusters.    ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\nd_dc_n = sqrtsum _i=1^l (x_m - x_c_n)^2\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"After assigning the data points to the first set of temporary clusters, the k-means algorithm adjusts the centroids by putting them in the center of clusters, only considering the actual data. At this stage the process of distance calculation is repeated again and the points may be reassigned to another cluster based on their distances. This process is repeated until either the location of centroids remain constant or there is no reassignment of the points. These are typical stopping signals for the k-means algorithm. ","category":"page"},{"location":"KMeans/#Practical-Example","page":"k-means","title":"Practical Example","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"Let's build a simple k-means algorithm together using some random data: ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"using ACS\n\nc1 = 5 .* rand(5)\ncx1 = 1 .+ (2-1) .* rand(5)\nc2 = 20 .* rand(5)\ncx2 = 6 .+ (10-6) .* rand(5)\n\ndata_s = vcat(hcat(cx1,c1),hcat(cx2,c2))\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\nscatter(data_s[:,1],data_s[:,2],label=false)\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"KMeans/#Step-1:-Selection-of-centroids","page":"k-means","title":"Step 1: Selection of centroids","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"For this case we have only two variables in our dataset data_s. For simplicity, we will start with two clusters, thus k is equal 2. In this case, we can either select two random points within the measurements' window (i.e. between minimum and maximum of values in data_s) or we can choose the indices of two of the measurements at random. Here we go with the second option.  ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"k = 2 # k is the number of clusters\n\nind_c = Int.(round.(1 .+ (size(data_s,1) - 1) .* rand(k))) # index of the centroids\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"scatter(data_s[:,1],data_s[:,2],label=\"data_s\",legend=:topleft)\nscatter!(data_s[ind_c,1],data_s[ind_c,2],label=\"Centroids\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n\n","category":"page"},{"location":"KMeans/#Step-2:-Calculation-of-distances","page":"k-means","title":"Step 2: Calculation of distances","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"To calculate the distances, we create a vector containing the distance of each centroid from every single point in the data_s. This will result in a distance matrix of d_10 times 2, where column one belongs to the first centroid and the first point corresponds to the first data point in data_s. Since we are using the actual measurements as our starting point, we will have zero distances for at least one point per column.","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\nd = zeros(size(data_s,1),length(ind_c)) # Generating the distance matrix\n\nfor i = 1:length(ind_c)\n    cent = transpose(data_s[ind_c[i],:])\n    d[:,i] = sqrt.(sum((cent .- data_s).^2,dims=2))\n\nend \n\nd\n","category":"page"},{"location":"KMeans/#Step-3-Assigning-the-points-to-each-cluster","page":"k-means","title":"Step 3 Assigning the points to each cluster","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"Now that we have our distance matrix d, we need to assign each point to a cluster, in our case the two clusters. To do so, we can look at our d matrix row wise and based on the column with the minimum distance assign that point to a cluster. For example, if in d[1,:] the minimum value is located at d[1,2], then this point belongs to the cluster number two. These calculations can be done using the following code. ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\nclusters = zeros(size(d))\n\nfor i = 1:size(d,1)\n    clusters[i,argmin(d[i,:])] = argmin(d[i,:])\nend \n        \n\nclusters\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"scatter(data_s[clusters[:,1] .>0,1],data_s[clusters[:,1] .>0,2],label=\"Cluster 1\",legend=:topleft)\nscatter!(data_s[clusters[:,2] .>0,1],data_s[clusters[:,2] .>0,2],label=\"Cluster 2\")\nscatter!(data_s[ind_c,1],data_s[ind_c,2],label=\"Centroids\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n\n","category":"page"},{"location":"KMeans/#Step-4-Adjusting-the-centroids","page":"k-means","title":"Step 4 Adjusting the centroids","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"Now that we have the first set of temporary clusters, we need to recalculate the centroids using the the data points that are assigned to a specific cluster. To do that we calculate the mean of data_s column wise where the values in the clusters matrix is different from zero. ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"cents = zeros(size(clusters,2),size(data_s,2))\n\nfor i=1:size(clusters,2)\n\n    tv = clusters[:,i]\n    #println(tv)\n    cents[i,:] = mean(data_s[tv .>0,:],dims=1)\n\nend \n\ncents\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"scatter(data_s[clusters[:,1] .>0,1],data_s[clusters[:,1] .>0,2],label=\"Cluster 1\",legend=:topleft)\nscatter!(data_s[clusters[:,2] .>0,1],data_s[clusters[:,2] .>0,2],label=\"Cluster 2\")\nscatter!(data_s[ind_c,1],data_s[ind_c,2],label=\"Centroids\")\nscatter!(cents[:,1],cents[:,2],label=\"New centroids\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"As you can see, the locations of the \"New centroids\" have changed. Depending on the starting points, the new locations may or may not be closer to the global minimum of the system. The reach such a point, we need to repeat this process multiple times until we do not see any changes in the location of the centroids and thus the reassignment of the points to different clusters. ","category":"page"},{"location":"KMeans/#Applications","page":"k-means","title":"Applications","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"The k-means algorithm is mainly used for the clustering of multivariate data. Typically, it follows an HCA or PCA to identify the number of clusters. Then that number is fed to the k-means to generate the clusters. When dealing with very large number of variables, k-means is not the best algorithm as it may converge to a local minimum rather than a global minimum. It is recommended to run the k-means algorithms multiple times to make sure about the robustness of the location of the centroids. In fact most of existing packages for k-means have this feature already built in them. ","category":"page"},{"location":"KMeans/#Packages","page":"k-means","title":"Packages","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"There are different implementations of k-means algorithm available through ACS.jl package. Below the two main implementations are discussed. ","category":"page"},{"location":"KMeans/#[Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#)","page":"k-means","title":"Clustering.jl","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"The function kmeans(-) is provided via ACS.jl. Please note that the matrix fed to the kmeans(-) should have the variables in rows and measurements in columns. This means that for making it work with our usual datasets, you need to transpose your matrix prior to the model building.   ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\nk_mod = kmeans(transpose(data_s),2) # to build a k-means model\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"Once, the model is built, you are able to get the points assigned to each cluster as well as the coordinates of the centroids. It should be noted that the coordinates of the centroids, similarly to our data are transposed. Thus each represents the coordinates for each centroid rather than the columns. ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"\na_m = assignments(k_mod) # to get the assignment of the points in the data based on the model\nc_c = k_mod.centers\n\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"We can also plot our results for visualization.","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"scatter(data_s[:,1],data_s[:,2],group=k_mod.assignments,legend=:topleft) \nscatter!(c_c[1,:],c_c[2,:],label=\"Centroids\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"It should be noted that the julia implementation of k-means algorithm does not have an apply(-) incorporated in it meaning that for new data the distances from centroids must be calculated manually for the cluster assignment. This is in-line with the unsupervised nature of the k-means algorithm that should not be used for inferences. ","category":"page"},{"location":"KMeans/#[sklearn.cluster.k_means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)","page":"k-means","title":"sklearn.cluster.k_means","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"Through ACS.jl package you can also use the python implementation of k-means via scikit-learn. In this case there is no need for transposing your data. Thus you can use your data as it is (i.e. columns for variables and rows for the measurements). The algorithm outputs a python object containing the coordinates of the centroids and the assigned cluster to each point. Here is an example for our data_s.","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"using Pkg\nPkg.add(\"ScikitLearn\")\n\nusing ScikitLearn\n\n@sk_import cluster: KMeans\n\nkmeans_ = KMeans(n_clusters=2, random_state=0).fit(data_s)\n\n\nkmeans_.cluster_centers_","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"We can also plot the results as shown below. Please note that in case of python implementation you can use the function apply(-) to perform prediction using the built model.","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"scatter(data_s[:,1],data_s[:,2],group=kmeans_.labels_,legend=:topleft)\nscatter!(kmeans_.cluster_centers_[:,1],kmeans_.cluster_centers_[:,2],label=\"Centroids\")\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"KMeans/#Additional-Example","page":"k-means","title":"Additional Example","text":"","category":"section"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"If you are interested in practicing more, you can use the NIR.csv file provided in the folder dataset of the package ACS.jl github repository. You can try to use the NIR spectra and k-means to see whether there are clear clusters of samples associated with different octane number. ","category":"page"},{"location":"KMeans/","page":"k-means","title":"k-means","text":"If you are interested in additional information about k-means and would like to know more you can check this MIT course material.  ","category":"page"},{"location":"DT_RF/#Decision-trees-and-random-forrest","page":"Decision trees","title":"Decision trees and random forrest","text":"","category":"section"},{"location":"DT_RF/#Introduction","page":"Decision trees","title":"Introduction","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"Decision trees are a modeling strategy for dealing with complex data that may include non-linearity and/or non-continuity. Decision trees can be used for solving both regression and classification problems. Decision trees are easy to interpret as you can follow the data within the tree and are able to include the information from different parameters into the final model prediction. The two main disadvantages of decision trees are that they tend to have low accuracy and biased towards categorical variables with more levels. These shortcomings of decision trees are mitigated by the use of random forest, which is an ensemble approach combining the prediction of multiple decision trees to generate the final prediction the model. Forest based models are generally considered black box approaches as they could quickly get very complicated to interpret. However, they are capable of producing reasonable prediction with lower probability of overfitting. ","category":"page"},{"location":"DT_RF/#How-to-decision-tree?","page":"Decision trees","title":"How to decision tree?","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"In decision trees there are three essential entities: root node, inner node, and leaf node. The root node is a splitting point in a one of the variables where the maximum information gain is obtained. In other word, the highest level of separation is usually obtained at the root node. The inner nodes or nodes are the splitting points for each variable, where over each iteration the purity of the data increases. Finally the leaf node or leaf is the node where further splitting will not result in accuracy gain. Usually the mean of the leaf is considered the model prediction for that specific node. The main formula used for finding a splitting point is the residual sum squares (RSS). Here we are looking for the minimum value of RSS to be used as the splitting point.    ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"\nRSS = sum ^n_i=1 (y_i - f(x_i))^2 \n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"In this equation f(x_i) represents the mean of the data points are grouped together while the y_i is the each individual measurement. By calculating the RSS we are evaluating whether the mean of grouped data points is a good enough estimation of the data. Once one set of data points are grouped, then we use their mean (average) to represent them. This process is repeated for all the variables and data points until every measurement is a relatively pure leaf. A pure leaf is a leaf where the mean of the grouped data points is an accurate estimation of most data points. Let's see how this works. ","category":"page"},{"location":"DT_RF/#Practical-Example","page":"Decision trees","title":"Practical Example","text":"","category":"section"},{"location":"DT_RF/#Univariate-Case","page":"Decision trees","title":"Univariate Case","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"To start we will use a two dimensional data set, where we would like to predict the relative intensity of our signal based on the injected mass of the calibrant into our mass spectrometer. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nfile_name = \"Rel_res.csv\"               # This file is available at: https://github.com/EMCMS/ACS.jl/tree/main/datasets\n\ndata = read_ACS_data(file_name)         # Importing the data using an ACS function\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=false)\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"As you can see, our measurements are not linear and/or continuous. Thus a simple linear model will not be an adequate estimator of this data. Also our data appears to consist of five clusters for each concentration range (i.e. independent variable). Let's try to model this data using a decision tree.","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=\"data\")\nplot!([0,90],[0,100],label=\"Linear model\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"The first step to build a decision tree model is to find the root node (i.e. the first splitting point). Given that we are looking at only one dependent variable (i.e. the relative intensity), we will need to find the first splitting point only in this variable. We will look at the bi-variate case later on. To find the first splitting point, we start with injected masses larger than 2 (i.e. all our measurements except the first one). This implies that we have two groups where the injected mass is either smaller than 2 or larger than two. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=\"data\")\nplot!([0,2],[6,6],label=\"< 2\")\nplot!([2,90],[mean(data[2:end,\"Signal \"]),mean(data[2:end,\"Signal \"])],label=\"> 2\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"Here we can visually see that the group \"< 2\" is providing very accurate prediction (RSS = 0) while the second group is not able to predict the instrument response properly. To quantify this, we will use the RSS value for each mass of calibrant being the splitting point. It should be that for the first iteration of the splitting, we will use the point itself as we cannot calculate the mean of a single number.","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nRSS_l = data[1,\"Signal \"] - data[1,\"Signal \"] # \n\nRSS_r = sum((data[2:end,\"Signal \"] .- mean(data[2:end,\"Signal \"])).^2)\n\nRSS = RSS_l + RSS_r\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"This process is repeated for every single breaking points. For example let's repeat these calculations for a mass of 44.3 Fg (i.e. 26th point).","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nsplit = 26\n\nRSS_l = sum((data[1:split,\"Signal \"] .- mean(data[1:split,\"Signal \"])).^2) \n\nRSS_r = sum((data[split:end,\"Signal \"] .- mean(data[split:end,\"Signal \"])).^2)\n\nRSS = RSS_l + RSS_r\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=\"data\")\nplot!([0,data[!,\"Conc\"][split]],[mean(data[1:split,\"Signal \"]),mean(data[1:split,\"Signal \"])],label=\"< 44.3\")\nplot!([data[!,\"Conc\"][split],90],[mean(data[split:end,\"Signal \"]),mean(data[split:end,\"Signal \"])],label=\"> 44.3\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"We can use the below for loop to calculate the RSS for all potential splitting points. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"warning: Warning\nPlease note that this code assumes that your data is sorted and thus it does not take into  account the actual injected masses (i.e. the independent variable). If you want a more generic solution, you need to take the independent variables into account when creating right and left sides!","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\n \nRSS = zeros(length(data[!,\"Signal \"]))\nrss_l = zeros(length(data[!,\"Signal \"]))\nrss_r = zeros(length(data[!,\"Signal \"]))\n\nfor i=1:length(data[!,\"Signal \"])\n    rss_l[i] = sum((data[1:i,\"Signal \"] .- mean(data[1:i,\"Signal \"])).^2) \n\n    rss_r[i] = sum((data[i:end,\"Signal \"] .- mean(data[i:end,\"Signal \"])).^2)\n\n    RSS[i] = rss_l[i] + rss_r[i]\nend \n\n\nscatter(data[!,\"Conc\"],RSS, label=\"RSS\",markersize = 5)\nscatter!(data[!,\"Conc\"],rss_l, label=\"RSS left\", markersize = 2)\nscatter!(data[!,\"Conc\"],rss_r, label=\"RSS right\", markersize = 2)\nxlabel!(\"Mass of calibrant Fg\")\nylabel!(\"RSS\")\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"In the above plot we are able to clearly see both the splitting point and the contribution of each side (i.e. the left vs right RSS). For example for all the points with injection mass of < 17, the model prediction has little to no error in it whereas the model is not accurate at all for injection masses > 17. At the moment, our model has only one splitting point.","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nsplit = 10\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=\"data\")\nplot!([0,data[!,\"Conc\"][split]],[mean(data[1:split,\"Signal \"]),mean(data[1:split,\"Signal \"])],label=\"< 17\")\nplot!([data[!,\"Conc\"][split],90],[mean(data[split:end,\"Signal \"]),mean(data[split:end,\"Signal \"])],label=\"> 17\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"After the first splitting, we need to decide whether each side require further splitting. If we look at the left side the value of 6 (i.e. the model prediction) is fairly accurate prediction. However, we can further split these measurements to get even more accurate estimations. This added accuracy also comes with a high potential for overfitting. There are several methods to assess the presence of overfitting in our model. These approaches include setting a depth limit, setting a minimum number of points to a leaf, information gain, and Gini impurity. Each of these approaches have their own advantages and disadvantages and you should decide the most adequate one, based on your knowledge of the data. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"tip: Tip\nThe most intuitive approach to avoid overfitting in tree based models is to set a minimum number of points in terminal leafs. This implies that if there are less than the set value points in a leaf that leaf is considered terminal and will not be considered for further splitting. This number (i.e. the minimum points per leaf) can be estimated by looking at the distribution of your data. Typically, the highest levels of accuracy and robustness is achieved by using a number between 5 and 20 points per leaf. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"Now that the root of our decision tree is identified and we have decided that the left leaf is not going to be further split into more leafs, we need to improve our model for the right side of the splitting point. To do so, we can exclude the left leaf from our data and repeat the RSS calculations for the rest of the data. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nconc_1 = data[11:end,\"Conc\"]\nsig_1 = data[11:end,\"Signal \"]\n \nRSS1 = zeros(length(conc_1))\nrss_l1 = zeros(length(conc_1))\nrss_r1 = zeros(length(conc_1))\n\nfor i=1:length(conc_1)\n    rss_l1[i] = sum((sig_1[1:i] .- mean(sig_1[1:i])).^2) \n\n    rss_r1[i] = sum((sig_1[i:end] .- mean(sig_1[i:end])).^2)\n\n    RSS1[i] = rss_l1[i] + rss_r1[i]\nend \n\n\nscatter(data[11:end,\"Conc\"],RSS1, label=\"RSS\",markersize = 5)\nscatter!(data[11:end,\"Conc\"],rss_l1, label=\"RSS left\", markersize = 2)\nscatter!(data[11:end,\"Conc\"],rss_r1, label=\"RSS right\", markersize = 2)\nxlabel!(\"Mass of calibrant Fg\")\nylabel!(\"RSS\")\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"As you can clearly see from the above plot the measurements larger than 68 Fg are grouped together and since the number of points in that leaf is smaller or equal to the set limit of 10 points, thus this is our second leaf. Currently our model has three potential predictions depending on the provided mass of the injected calibrant. For the two edge leafs, our model does very well in terms of accuracy while for the center leaf further splitting is necessary. We can perform this by repeating the above steps.  ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nsplit1 = 10\nsplit2 = 40\n\n\nscatter(data[!,\"Conc\"],data[!,\"Signal \"], label=\"data\")\nplot!([0,data[!,\"Conc\"][split1]],[mean(data[1:split1,\"Signal \"]),mean(data[1:split1,\"Signal \"])],label=\"< 17\")\n\nplot!([data[!,\"Conc\"][split1],data[!,\"Conc\"][split2]],[mean(data[split1:split2,\"Signal \"]),mean(data[split1:split2,\"Signal \"])],label=\"17 < x  < 68\")\n\nplot!([data[!,\"Conc\"][split2],90],[mean(data[split2:end,\"Signal \"]),mean(data[split2:end,\"Signal \"])],label=\"> 68\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n\n","category":"page"},{"location":"DT_RF/#Multivariate-Case","page":"Decision trees","title":"Multivariate Case","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"So far we have only looked at a univariate case. For a system with multiple variables, the process is the same as the univariate one and it is performed over each variable separately and then the minimum RSS value by the splitting points are compared, the variable with the smallest RSS value will take precedent. For example in case of our dataset, now we can use both variables in our model. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\nscatter(data[!,\"Conc\"][data[!,\"Mode\"] .== 1],data[!,\"Signal \"][data[!,\"Mode\"] .== 1], label=\"Positive mode\")\nscatter!(data[!,\"Conc\"][data[!,\"Mode\"] .== -1],data[!,\"Signal \"][data[!,\"Mode\"] .== -1], label=\"Negative mode\")\nxlabel!(\"Mass calibrant (Fg)\")\nylabel!(\"Relative intensity (%)\")\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"In this case, the mass of the calibrant will still be the root node of our tree while the second node will be related to the acquisition mode.","category":"page"},{"location":"DT_RF/#Random-Forest","page":"Decision trees","title":"Random Forest","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"As it was mentioned above random forest was introduced as a means for overcoming the issues associated with the decision trees namely the lack of accuracy and robustness. The basic concept behind random forest is the use of multiple trees and the resampling strategies such as bootstrapping. Here we first generate a large number of trees (e.g. hundreds) and with each of those trees we model a bootstrap sample of the original data. This implies that no two trees have the same dataset to model and our random forest model has as many prediction as the number of trees. At this point the random forest model tallies the distribution of the predictions and outputs the prediction with the highest likelihood (i.e. the most frequent prediction). This strategy, usually, results in a much more robust and accurate models, mitigating the limitations of decision trees. When building random forest models, one can decide how to distribute the data across the trees. For example, one of the most communly used approaches to build bootstrapping samples is bagging, where not all the variables and the measurements are given to each tree. When performing random forest modeling, independently from the used package, there are three hyperparameters that must be optimized namely: the number of trees, the minimum points per leaf, and bootstrapping conditions. Each of these parameters have their own criterion for being optimized and thus they need to be considered all together. For example, we want to keep the number of trees to the minimum while having highest possible accuracy of the cross validation and test set. One way to deal with this is to perform grid search, which could be computationally expensive, depending on the resolution needed. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"tip: Tip\nFor the number of trees usually you are looking for the optimized number between 100 and 500 trees. Please note this is highly case dependent and may not be correct for your specific dataset. For the bootstrapping parameter, in case of classification a square root of the number of variables and for regression 1/3 of the number of variable can be used as starting points. ","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"tip: Tip\nWhen building any type of model, you need to make sure that the variables are scaled. Otherwise, the variable with the largest magnitude will dominate your model, even though, it may not be the most relevant variable to the model.","category":"page"},{"location":"DT_RF/#Packages","page":"Decision trees","title":"Packages","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"The package ScikitLearn.jl which is a julia wrapper of the python package provides access to a wide variety of random forest implementations. Below is an example of the usage case for a classification problem.","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"using ACS\n\n@sk_import ensemble: RandomForestClassifier\n\ndata = dataset(\"datasets\", \"iris\")\n\ny = data[!,\"Species\"]\nY = zeros(length(y))\nY[y .== \"setosa\"] .= 1\nY[y .== \"versicolor\"] .= 2\nX = Matrix(data[:,1:4]); # The first four columns are selected for this\n\nclf = RandomForestClassifier(n_estimators=200, min_samples_leaf=5,\noob_score =true, max_features= \"auto\",n_jobs=-1).fit(X,Y)\naccuracy_cl = clf.score(X,Y)\n","category":"page"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"warning: Warning\nPlease note that the above implementation is not following the best practices for this type of modeling as the data is not split into training set and test set. Also, no cross-validation is performed. Finally none of the hyperparameters here have been optimized.  ","category":"page"},{"location":"DT_RF/#Additional-Resources","page":"Decision trees","title":"Additional Resources","text":"","category":"section"},{"location":"DT_RF/","page":"Decision trees","title":"Decision trees","text":"There are several resources (including videos on YouTube) for better understanding how decision trees and random forests work. The documentation of SKLearn package is one of the best resources for this. Additionally, you can follow this lecture by Kilian Weinberger from Cornell. ","category":"page"},{"location":"KNN/#*k*-nearest-neighbors-algorithm-(*k*-NN)","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"","category":"section"},{"location":"KNN/#Introduction","page":"k-nearest neighbors algorithm (k-NN)","title":"Introduction","text":"","category":"section"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"The k-NN is a non-parametric and supervised algorithm for both regression and classification of multidimensional data. The k-NN can handle both numerical and categorical datasets to perform the regression or classification.   ","category":"page"},{"location":"KNN/#How?","page":"k-nearest neighbors algorithm (k-NN)","title":"How?","text":"","category":"section"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"The principal concept behind k-NN is to use the average estimate of k nearest neighbors as the model prediction for that specific measurement, implying that the model relies purely on the underlying trend in the data itself (i.e. non generalized function or equation). The steps essential to k-NN are: (1) distance calculations, (2) sorting of the data (3) finding k nearest neighbors, and (4) estimate the model output using those data.  ","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"As mentioned above k-NN is a supervised algorithm, thus the data provided to the algorithm must have X matrix (i.e. the independent variables) and Y matrix/vector (i.e. dependent variable). The simplest example to start with is a simple calibration problem. Let's generate the needed data for our example. ","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"using ACS\n\nf(x) = 2 .* x .+ 5  \t\t# Defining the function\nx = collect(1:0.1:10)     \t# Defining X\nx_n = rand(length(x))\t\t# Let's generate some noise\n\ny = f(x .+ x_n)\t\t\t\t# Generate the Y via the function X and noise\n\n# Plotting the data\nscatter(x ,y,label = false)\nxlabel!(\"X\")\nylabel!(\"Y\")\n","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"This dataset can be fitted with a linear model using the least square providing us with the associated coefficients. Let's assume we do not know least squares and want to solve this using k-NN to perform this regression. As mentioned above, we follow the below steps:","category":"page"},{"location":"KNN/#Step-1:","page":"k-nearest neighbors algorithm (k-NN)","title":"Step 1:","text":"","category":"section"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"To calculate the distances, we will use the Euclidean distance. Other distance metrics are also possible but for this example we will work the simplest one. For the distance calculations we ignore the Y matrix and only focus on the X matrix. ","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"using ACS\n\ndist = dist_calc(hcat(x,x))\n\n# Plotting the data\nheatmap(dist,label = false)\n\n","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"As expected the values in the distance matrix are directly correlated with the X_i, which is caused by the fact that our values in the X matrix are sorted. For the next step we will set the k value to 5, suggesting that our model will use 5 closest entries to estimate the value of that query. To do this we need to first set the diagonal of our distance matrix to \"Inf\" as it is currently filled with zeros. ","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"using ACS\n\ndist_ = deepcopy(dist)\n\ndist_[diagind(dist_)] .= Inf                      # Set the diagonal to inf, which is very helpful when searching for minimum distance\n\n# Plotting the data\nheatmap(dist_,label = false)\n\n","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"Then we will sort our distance matrix to combine the measurements into groups of 5. Here we can use the below code to perform the grouping of our data. The function sortperm(-) is the way to go.","category":"page"},{"location":"KNN/","page":"k-nearest neighbors algorithm (k-NN)","title":"k-nearest neighbors algorithm (k-NN)","text":"using ACS\n\np = sort(dist)\n\ndist_[diagind(dist_)] .= Inf                      # Set the diagonal to inf, which is very helpful when searching for minimum distance\n\n# Plotting the data\nheatmap(dist_,label = false)\n\n","category":"page"},{"location":"KNN/#Practical-Application","page":"k-nearest neighbors algorithm (k-NN)","title":"Practical Application","text":"","category":"section"},{"location":"KNN/#Additional-Example","page":"k-nearest neighbors algorithm (k-NN)","title":"Additional Example","text":"","category":"section"},{"location":"prep/#Julia-Programming-Language","page":"Preparation","title":"Julia Programming Language","text":"","category":"section"},{"location":"prep/#Installation","page":"Preparation","title":"Installation","text":"","category":"section"},{"location":"prep/#Julia","page":"Preparation","title":"Julia","text":"","category":"section"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"1.\tDownload the long term support (LTS) release for Julia from: https://julialang.org/downloads/","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"2.\tExecute the file and follow the installation steps. Make sure to write down the installation path or use the default path (This information will be required in a later step).","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"(Image: julia_down)","category":"page"},{"location":"prep/#Visual-Studio-Code","page":"Preparation","title":"Visual Studio Code","text":"","category":"section"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"1.\tDownload and install Visual Studio Code (VScode) from: https://code.visualstudio.com/download","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"2.\tTo open the extension marketplace, press (Ctrl+Shift+X) or press the marketplace button on the left side of the screen","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"3.\tSearch for julia. ","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"4.\tSelect the first package named Julia and install. ","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"5.\tAfter installation restart VS Code for the next steps.","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"(Image: julia_ex)","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"To ensure that VS Code can find the installed Julia language:","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"1.\tGo to: file -> preferences -> settings. ","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"2.\tSearch for the Julia.executablePath (Figure below).","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"3.\tFill in the path (i.e., installation path noted before) to the Julia executable (Julia.exe). ","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"In case you do not have the path you can do the following:\nDefault path for Windows: C:\\Users\\[INSERTUSERNAME]\\AppData\\Local\\Programs\\Julia1.5.3\\bin\\julia.exe\nFor windows, double slashes (\\) need to be used instead of single () because the single slash is interpreted as escape.","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"To locate appdata folder, you can press the windows key and type: %appdata%\nCopy path and add \\Local......:\n[Copied path ending with \\AppData]\\Local\\Programs\\Julia-x.x.x\\bin \\julia.exe","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"Default path for Mac: \nMacintoshHD/users/[INSERTUSERNAME]/Applications/Julia-x.x.x.app/Contents/Resources/julia/bin/Julia","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"(Image: julia_path)","category":"page"},{"location":"prep/#Julia-package-installation","page":"Preparation","title":"Julia package installation","text":"","category":"section"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"To install packages we first need to start up Julia. To open the Julia REPL for executing commands press Crtl+Shift+P and search for Julia: Start REPL and press enter, opening up the following window:","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"(Image: REPL)","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"In the REPL execute the following lines of code in order to install packages. After a single command is executed, “julia >” re-appears and the next line can be executed (it might take a bit for the packages to finish downloading). Also, the lines are capital sensitive. In case there is a space (\" \") in the user name, see the next section for installing the ACS package.","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"using Pkg\n# Julia official packages\nPkg.add(\"CSV\")\n\n# Julia unofficial packages from repository\nPkg.add(PackageSpec(url=\"https://github.com/EMCMS/ACS.jl\"))","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"The second package that is being installed contains all relevant functions and information for the Advanced Chemometrics and Statistics course.","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"tip: Tip\nFor basic julia programming please check our tutorials as a part of the documentation of package DataSci4Chem.jl. ","category":"page"},{"location":"prep/#Installing-the-ACS-package-with-\"-\"-in-username","page":"Preparation","title":"Installing the ACS package with \" \" in username","text":"","category":"section"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"The ACS package makes use of other packages, which includes Conda. However, the Conda package cannot install itself and will error if there is a space (\" \") in the username (i.e., the path where Conda wants to install itself). Therefore, the following steps need to be followed to get Conda working. Make sure to carefully read the comments that follow after a #.","category":"page"},{"location":"prep/","page":"Preparation","title":"Preparation","text":"using Pkg\n# Download Conda\nPkg.add(\"Conda\")\n# Start up\nusing Conda\n# Change the installation path of Conda\nENV[\"CONDA_JL_HOME\"] = \"C:\\\\Conda-Julia\"      #default/example path\n\n\n# Now, before we can continue, make sure that this folder exist. Especially if a different path is chosen then the default.\n# To create the folder in the default path run:\nmkdir(\"C:\\\\Conda-Julia\")\n\n\n# Download PyCall package that uses Conda -> Ignore the error!\nPkg.add(\"PyCall\")\nPkg.build(\"PyCall\")\n\n\n# Install the ACS package\nPkg.add(PackageSpec(url=\"https://github.com/EMCMS/ACS.jl\"))\n\n#!!! re-start Julia !!!\n\n# Now the ACS package can be used through:\nusing ACS\n","category":"page"},{"location":"svd/#Singular-Value-Decomposition-(SVD)","page":"SVD","title":"Singular Value Decomposition (SVD)","text":"","category":"section"},{"location":"svd/#Introduction","page":"SVD","title":"Introduction","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"The SVD is a matrix factorization technique that decomposes any matrix to a unique set of matrices. The SVD is used for dimension reduction, trend analysis, and potentially for the clustering of a multivariate dataset. SVD is an exploratory approach to the data analysis and therefore it is an unsupervised approach. In other words, you will only need the X block matrix. However, where the Y matrix/vector is available, it (i.e. Y) can be used for building composite models or assess the quality of the clustering. ","category":"page"},{"location":"svd/#How?","page":"SVD","title":"How?","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"In SVD the matrix X_m times n is decomposed into the matrices U_m times n, D_n times n, and V_n times n^T. The matrix U_m times n is the left singular matrix and it represents a rotation in the matrix space. The D_n times n is diagonal matrix and contains the singular values. This matrix may be indicated with different symbols such as Sigma_n times n. The D_n times n matrix in the geometrical space represents an act of stretching. Each singular value is the degree and/or weight of stretching. We use the notation D_n times n to remind ourselves that this is a diagonal matrix. Finally, V_n times n^T is called the right singular matrix and is associated with rotation. Overall, SVD geometrically is a combination of a rotation, a stretching, and a second rotation.","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"The two matrices U_m times n and V_n times n^T are very special due to their unitary properties.","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nU^T times U = U times U^T = I\nV^T times V = V times V^T = I\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Therefore the general matrix expression of SVD is the following: ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"X = UDV^T\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"To deal with the non-square matrices, we have to convert our X matrix to X^T times X. This implies that our SVD equation will become the following: ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"X^TX = (UDV^T)^T times UDV^T\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"And after a little bit of linear algebra: ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"X^TX = VD^T times DV^T  \nand \n\nXV = UD\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"This is a system of two equations with two variables that can be solved. Before looking at an example of such system let's remind ourselves that VD^T times DV^T is the solution of eigenvalue/eigenvector decomposition of X^TX. This means that both D and V^T can be calculated by calculating the eigenvalues and eigenvectors of X^TX. Therefore we can calculate D and V as follows:","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nD = sqrteigenvalues(X^TX) \nV = eigenvector(X^TX)\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Once we know V, we can use that and the second equation of SVD to calculate the last part i.e. the matrix U. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"U = XVD^-1 \n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Please note that D^-1 denotes the inverse or pseudo-inverse of the matrix D.  ","category":"page"},{"location":"svd/#Practical-Example","page":"SVD","title":"Practical Example","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"Let's do the SVD calculations together for the below matrix: ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"using ACS\n\nX = [5 -5;-1 7;1 10]\n","category":"page"},{"location":"svd/#Step-1:-X{T}X","page":"SVD","title":"Step 1: X^TX","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"# The function transpose(-) is part of LinearAlgebra.jl package that has been automatically installed via ACS.jl package.\n# Not all the functions of LinearAlgebra.jl are exported within the ACS.jl environment. \nXtX = transpose(X)*X \n","category":"page"},{"location":"svd/#Step-2:-Calculation-of-*D*,-*V*,-and-*U*","page":"SVD","title":"Step 2: Calculation of D, V, and U","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nD = diagm(sqrt.(eigvals(XtX))) # A diagonal matrix is generated\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nV = eigvecs(XtX) # Right singular matrix\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nU = X*V*pinv(D)\t# Left singular matrix\n\n","category":"page"},{"location":"svd/#Builtin-Function","page":"SVD","title":"Builtin Function","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"The same calculations can be done with the function svd(-) of ACS package provided via LinearAlgebra.jl package. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n out = svd(X)\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n D = diagm(out.S) # The singular value matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n U = out.U # Left singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n V = transpose(out.Vt) # Right singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Please note that the builtin function sorts the singular values in descending order and consequently the other two matrices are also sorted following the same. Additionally, for ease of calculations the builtin function generates the mirror image of the U and V matrices. These differences essentially do not impact your calculations at all, as long as they are limited to what is listed above.","category":"page"},{"location":"svd/#Step-3-Calculation-of-\\hat{X}","page":"SVD","title":"Step 3 Calculation of hatX","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"Using both the manual method and the builtin function, you can calculate hatX following the below operation. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nX_hat = U*D*transpose(V)\n","category":"page"},{"location":"svd/#Applications","page":"SVD","title":"Applications","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"As mentioned above SVD has several applications in different fields. Here we will focus on three, namely: dimension reduction, clustering/trend analysis, and multivariate regression. This dataset contains five variables (i.e. columns) and 150 measurements (i.e. rows). The last variable \"Species\" is a categorical variable which defines the flower species. ","category":"page"},{"location":"svd/#Dimension-Reduction","page":"SVD","title":"Dimension Reduction","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"To show case the power of SVD in dimension reduction we will use the Iris dataset from Rdatasets. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"using ACS\n\ndata = dataset(\"datasets\", \"iris\")\ndescribe(data) # Summarizes the dataset\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Here we show how SVD is used for dimension reduction with the iris dataset. First we need to convert the dataset from table (i.e. dataframe) to a matrix. For data we can use the function Matrix(-) builtin in the julia core language.","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Y = data[!,\"Species\"]\nX = Matrix(data[:,1:4]); # The first four columns are selected for this\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Now we can perform SVD on the X and try to assess the underlying trends in the data. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n out = svd(X)\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n D = diagm(out.S) # The singular value matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n U = out.U # Left singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n V = transpose(out.Vt) # Right singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"As you may have noticed, there are four variables in the original data and four non-zero singular values. Each column in the lift singular matrix is associated with one singular value and one row in the V matrix. For example the first column of sorted U matrix (i.e. via the builtin function) is directly connected to the first singular value of 95.9 and the first row of the matrix V. With all four singular values we can describe 100% of variance in the data (i.e. hatX = X). By removing the smaller or less important singular values we can reduce the number of dimensions in the data. We can calculate the variance explained by each singular value via two different approaches. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n var_exp = diag(D) ./ sum(D) # diag() selects the diagonal values in a matrix \n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n var_exp_cum = cumsum(diag(D)) ./ sum(D) # cumsum() calculates the cumulative sum \n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n scatter(1:length(var_exp),var_exp,label=\"Individual\")\n plot!(1:length(var_exp),var_exp,label=false)\n\n scatter!(1:length(var_exp),var_exp_cum,label=\"Cumulative\")\n plot!(1:length(var_exp),var_exp_cum,label=false)\n xlabel!(\"Nr Singular Values\")\n ylabel!(\"Variance Explained\")\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Given that the first two singular values explain more than 95% variance in the data, they are considered enough for modeling our dataset. The next step here is to first plot the scores (i.e. the left singular matrix) of first and second singular values against each other to see whether we have a model or not. Each column in the U matrix represents a set of scores associated with a singular value (e.g. first column for the first singular value).","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n scatter(U[:,1],U[:,2],label=false)\n xlabel!(\"First Singular value (81%)\")\n ylabel!(\"Second Singular value (15%)\")\n \n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"At this point we are assuming that we do not have any idea about the plant species included in our dataset. Now we need to connect the singular values to individual variables. For that similarly to PCA we will take advantage of the loadings, which in this case are the columns of the V or the rows of V^T. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n bar(V[:,1],label=\"First SV\")\n bar!(V[:,2],label=\"Second SV\")\n xlabel!(\"Variable Nr\")\n ylabel!(\"Importance\")\n #ylims!(-0.1,0.1)\n\n \n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"The sign of each loading value shows the relationship between the variable and the model. For example, based on the first SV the variable number one and two both have a negative impact on the final model (i.e. scores of the SV1). A positive impact indicates an increase of the final model scores with the variable while a negative impact means a decrease in the score values with an increase the variable. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n p1 = scatter(X[:,1],U[:,1],label=false)\n xlabel!(\"SepalLength\")\n ylabel!(\"Scores U1\")\n\n  p2 = scatter(X[:,2],U[:,1],label=false)\n xlabel!(\"SepalWidth\")\n ylabel!(\"Scores U1\")\n\n  p3 = scatter(X[:,2],U[:,2],label=false)\n xlabel!(\"SepalWidth\")\n ylabel!(\"Scores U2\")\n\n p4 = scatter(X[:,3],U[:,2],label=false)\n xlabel!(\"PetalLength\")\n ylabel!(\"Scores U2\")\n\n plot(p1,p2,p3,p4,layout = (2,2))\n \n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"In this particular case, the SV1 is a linear combination of SepalLength and SepalWidth while the SV2 is a linear combination of all four variables. This implies that we can cover the variance present in the X with two variables, which are U1 and U2. For this dataset, we have a reduction of variables from 4 to 2, which may not look impressive. However, this can be a very useful technique when dealing with a large number of variables (the octane example).","category":"page"},{"location":"svd/#Clustering","page":"SVD","title":"Clustering","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"When we perform cluster analysis or most modeling approaches, we need to divide our data into training and test sets. We usually go for a division of 80% for training set and 20% for the test. More details are provided in the cross-validation chapter. Let's randomly select 15 data points to put aside as the test set. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\nn = 15 # number of points to be selected\n\nrand_ind = rand(1:size(X,1),n) # generate a set of random numbers between 1 and size(X,1)\nind_tr = ones(size(X,1))       # generate a matrix of indices \nind_tr[rand_ind] .= 0          # set the test set values' indices to zero \nX_tr = X[ind_tr .== 1,:]       # select the training set\nX_ts = X[rand_ind,:]           # select the test set\ndata[rand_ind,:]\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Now that we have training and test sets separated, we can build our model using the training set. This implies that the model has never seen the values in the test set. It should be noted that we always want the homogenous distribution of measurements in the test set. Also, each iteration here will result in a different test set as a new set of random numbers are generated.  Now let's build our model with only the X_tr following the same procedure as before. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n out = svd(X_tr)\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n D = diagm(out.S) # The singular value matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n U = out.U # Left singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n V = transpose(out.Vt) # Right singular matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Let's plot our results for the first two SVs, as we did before. However, this time we will take the knowledge of the different species into account. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n var_exp = diag(D) ./ sum(D) # variance explained \n Y_tr = data[ind_tr .== 1,\"Species\"]\n Y_ts = data[ind_tr .== 0,\"Species\"]\n scatter(U[:,1],U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = Y_tr)\n xlabel!(\"First Singular value (81%)\")\n ylabel!(\"Second Singular value (14%)\")\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"As it can be seen, this model is very similar to our previous model based on the full dataset. Now we need to first define thresholds for each class based on the score values in the U1 and U2 space. This is typically more difficult to assess. However, for this case the main separating factor is the U2 values (e.g. U2 geq 005 = Setosa). ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n scatter(U[:,1],U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = Y_tr)\n plot!([-0.15,0],[0.05,0.05],label=\"Setosa\")\n plot!([-0.15,0],[-0.04,-0.04],label=\"Virginica\")\n xlabel!(\"First Singular value (81%)\")\n ylabel!(\"Second Singular value (14%)\")\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"The next step is to calculate the score values for the measurements in the test set. This will enable us to estimate the class associated with each data point in the test set. To do this we need to do a little bit of linear algebra.","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"X = UDV^T\n\nU_test = X times (DV^T)^-1\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"In practice:","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n U_test = X_ts * V * inv(D)\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n scatter(U[:,1],U[:,2],label=[\"Setosa\" \"Versicolor\" \"Virginica\"], group = Y_tr)\n plot!([-0.15,0],[0.05,0.05],label=\"Setosa\")\n plot!([-0.15,0],[-0.04,-0.04],label=\"Virginica\")\n scatter!(U_test[:,1],U_test[:,2],label=\"Random points\",marker=:star,ms=10,alpha=0.5)\n xlabel!(\"First Singular value (81%)\")\n ylabel!(\"Second Singular value (14%)\")\n\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"As it can be seen from the results of the test set, our model is doing well for most cases. It should be noted that steps such as data pre-treatment and the use of supervised methods may improve the results of your cluster analysis. The use of SVD for prediction is not recommended. It must be mainly used for the dimension reduction and data exploration.  ","category":"page"},{"location":"svd/#Regression","page":"SVD","title":"Regression","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"If you have a dataset (e.g. octane dataset in the additional example), where the SVD is used to reduce the dimensions of the dataset. In this case the we can perform a least square regression using the selected columns of U rather than the original X. For example in case of iris dataset the U1 and U2 can be used to replace X.  ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":" X_svr = U[:,1:m] # m is the number of selected SVs \n Y_svr            # does not exist for iris dataset. for the octane dataset is the octane column\n b = pinv(transpose(X_svr) * X_svr) * transpose(X_svr) * Y_svr # simple least square solution\n y_hat = X_svr * b # prediction the y_hat ","category":"page"},{"location":"svd/#Trend-Analysis","page":"SVD","title":"Trend Analysis","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"We also can assess the trend represented by each SV in our model. This is typically done by setting all SV values except one to zero. Then the new D is used to predict hatX. Then different variables are plotted against each other for both X matrices. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":" D_temp = out.S\n D_temp[2:end] .= 0\n D_n = diagm(D_temp) # the new singular value matrix\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Then the hatX is calculated.","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":" X_h  = U * D_n * transpose(V)\n X_h[1:5,:]\n","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"Now if we plot the SepalLength vs SepalWidth we can clearly see a clear 1 to 2 relationship between the two variables which is being detected by the first SV. This can be done for other variables and SVs. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"\n scatter(X[:,1],X[:,2],label=\"X\")\n scatter!(X_h[:,1],X_h[:,2],label=\"X_h\")\n xlabel!(\"SepalLength\")\n ylabel!(\"SepalWidth\")\n\n","category":"page"},{"location":"svd/#Additional-Example","page":"SVD","title":"Additional Example","text":"","category":"section"},{"location":"svd/","page":"SVD","title":"SVD","text":"If you are interested in practicing more, you can use the NIR.csv file provided in the folder dataset of the package ACS.jl github repository. Please note that this is an SVR problem, where you can first use SVD for the dimension reduction and then use the selected SVs for the regression. ","category":"page"},{"location":"svd/","page":"SVD","title":"SVD","text":"If you are interested in math behind SVD and would like to know more you can check this MIT course material.  ","category":"page"},{"location":"#[Advanced-Chemometrics-and-Statistics-(ACS)](https://coursecatalogue.uva.nl/xmlpages/page/2022-2023-en/search-course/course/99444)","page":"Home","title":"Advanced Chemometrics and Statistics (ACS)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A documentation built for the ACS course to help the students during the lectures and for exam preparation ","category":"page"},{"location":"#Course-content","page":"Home","title":"Course content","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This course is built for MSc of Analytical (Chemistry) Sciences at the University of Amsterdam (UvA). In this course, the students learn how to handle/model multi-dimensional data from simple visualization to machine learning based modeling and inference. The course contains 11 lectures from which 8 of them are tackling independent topics while the remaining are more generic. For 6 out of those 8, separate pages are created within this documentation. In future more topics will be added to the package and thus the documentation. This package utilizes wide range of other packages developed by others, as this is meant to facilitate the introduction of the students to programming and data science. The package is based on julia language. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Introduction to julia and Jupyter notebook\nSingular value decomposition (SVD)\nMCR-ALS\nPartial least square regression (PLS-R)\nHierarchical Cluster Analysis (HCA)\nK-means clustering \nDecision trees and random forest \nAdvanced signal processing \nValidation and cross-validation \nBayesian statistics I \nBayesian statistics II","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more information about the course and us, please visit us at https://emcms.info. ","category":"page"},{"location":"Bayes/#Bayesian-Statistics","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"","category":"section"},{"location":"Bayes/#Introduction","page":"Bayesian Statistics","title":"Introduction","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Bayesian Statistics is a statistical approach based on the Bayes Theorem, where the probability of an event is expressed as the degree of belief. A major difference between Bayesian Statistics and the frequentist approach is the inclusion of the prior knowledge of the system into the probability calculations. One of the main differences between Bayesian Statistics and conventional t-test or ANOVA is that the Bayesian Statistics considers the probability of both sides of the null-hypothesis. ","category":"page"},{"location":"Bayes/#Bayes-Theorem","page":"Bayesian Statistics","title":"Bayes Theorem","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"In Bayes Theorem the probability of an even occurring is updated by the degree of belief (i.e. prior knowledge). The Bayes Theorem is mathematically express as follows: ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B) = fracP(B mid A)P(A)P(B)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"The term P(A mid B) is the posterior probability of A given B, implying that A and B are true given that B is true. The second term in this formula is the conditional probability of P(B mid A). The term P(A) is defined as the prior probability, which enables the incorporation of prior knowledge into the probability distribution of even occurring. Finally, P(B) is the marginal probability, which is used as a normalizing factor in this equation and it must be > 0. To put these terms into context, let's look at the following example. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"There is a new test for the detection of a new variant of COVID-19. We want to calculate the probability of a person being actually sick given a positive test (i.e. the posterior probability P(A mid B)). The conditional probability (P(B mid A)) in this case is the rate of true positive for the test. In other words the percentage of sick people who tested positive. The prior probability (P(A)) in this case is the probability of people getting sick while the marginal probability (P(B)) is all people who test positive independently from their health status.   ","category":"page"},{"location":"Bayes/#Bayes-Formula","page":"Bayesian Statistics","title":"Bayes Formula","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"The derivation of Bayes Theorem is very simple and can be done with simple knowledge of probability and arithmetics. Let's first write the two conditional probabilities (i.e. the posterior and conditional probabilities) as functions of their joint probabilities. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B) = fracP(A cap B)P(B) \nP(B mid A) = fracP(B cap A)P(A)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Given that P(A cap B) = P(B cap A) and is is unknown, one can solve the above equations as a function of this unknown joint probability. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A cap B) = P(A mid B)P(B) \nP(A cap B) = P(B mid A)P(A)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"This means that the right sides of these equations can be assumed equal as the left sides are equal, thus: ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B)P(B) = P(B mid A)P(A)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Now if we divide both sides of this equation by P(B), we will end up with the Bayes Theorem.","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B) = fracP(B mid A)P(A)P(B)\n","category":"page"},{"location":"Bayes/#Practical-Example","page":"Bayesian Statistics","title":"Practical Example","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"This is a very simple example where we work together to calculate the joint probabilities which are needed for the conditional probability calculations. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"We have a classroom with 40 students. We asked them to fill out a survey about whether they like football, rugby, or none of them. The results of our survey show that 30 students like football, 10 students like rugby, and 5 do not like these sports. As you can see, the total number of votes is larger than 40, implying that 5 students like both sports.  ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS\n\nplot([1,1],[1,6],label=false,color = :blue)\nplot!([1,9],[1,1],label=false,color =:blue)\nplot!([9,9],[1,6],label=false,color =:blue)\nplot!([1,9],[6,6],label=\"All students\",color =:blue)\nplot!([3,3],[1,6],label=false,color =:red)\nplot!([1,3],[3.5,3.5],label=false,color =:green)\nplot!([3,5],[3.5,3.5],label=false,color =:orange)\nplot!([5,5],[3.5,6],label=false,color =:orange)\nannotate!(2,5,\"Rugby Only\")\nannotate!(4,5,\"Both\")\nannotate!(2,2,\"None\")\nannotate!(6,2,\"Football Only\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Now lets generate the associated contingency table based on the survey results. Our table is a two by two one, given the number of questions asked. The contingency table will help us to calculate the joint probabilities. The structure of our table will be the following:","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":" Y Football N Football\nY Rugby  \nN Rugby  ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"After filling the table with the correct frequencies, we will end up with the following: ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":" Y Football N Football\nY Rugby 5 5\nN Rugby 25 5","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"These numbers can also be expressed in terms of probabilities rather than frequencies. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":" Y Football N Football\nY Rugby 5/40 = 0.125 5/40 = 0.125\nN Rugby 25/40 = 0.625 5/40 = 0.125","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"We can now further expand our table to include total cases of football and rugby fans amongst the students. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":" Y Football N Football Rugby total\nY Rugby 5/40 = 0.125 5/40 = 0.125 10/40 = 0.25\nN Rugby 25/40 = 0.625 5/40 = 0.125 30/40 = 0.75\nFootball total 30/40 = 0.75 10/40 = 0.25 ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"We can use the same table for calculating conditional probabilities, for example P(A mid B) or expanded to P(A  B mid B). Let's remind ourselves the formula for this: ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B) = fracP(A cap B)P(B)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"For this example we want to calculate the probability of a student liking football given that they are carrying a rugby ball (i.e. they like rugby). So now we can write the above formula using the annotation related to this question. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(Football  Rugby mid Rugby) = fracP(Football cap Rugby)P(Total Rugby)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Now we can plug in the numbers from our contingency table to calculate the needed probability. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(Football  Rugby mid Rugby) = frac0125025 = 05\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Another way to express these probabilities is using the probability trees. Each branch in a tree represents one set of conditional probabilities.   ","category":"page"},{"location":"Bayes/#Applications","page":"Bayesian Statistics","title":"Applications","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Bayesian Statistics has several applications from uncertainty assessment to network analysis as well as simple regression and classification. Here we will discuss  classification (i.e. Naive Bayes), uncertainty assessment, and regression. ","category":"page"},{"location":"Bayes/#Naive-Bayes","page":"Bayesian Statistics","title":"Naive Bayes","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Imagine you are measuring the concentration of a certain pharmaceutical in the urine samples of a clinical trial. This drug at low concentrations (i.e. < 5 ppb) does not have an effect while at high levels (i.e. > 9) could be lethal. Your previous measurements of these concentrations results in the below distribution.   ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nx = [2.11170571,4.654611177,2.058684377,9.118890998,6.482271164,1.741767743,0.423550831,3.930361297\n,8.394899978,2.720184918,4.642679068,0.698396604,10.60195845,9.949609087,9.788688087,9.275078609\n,3.71104968,3.048191598,7.131314198,2.696493503]\n\nhistogram(x,bins=5,normalize=:probability,label=false)\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"If we overlay the two concentration distributions assuming Normality, we will end up with the below figure. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,normalize=:probability,label=false)\nplot!(Normal(9.5,0.7*std(x)),label=\"Toxic\")\nplot!(Normal(2.5,std(x)),label=\"No effect\",c=:black)\n\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Now you are given a new sample to measure for the concentration of the drug. Your boss is only interested in whether this new sample is a no effect or a toxic case. After your measurement you will end up with the below results. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,normalize=:probability,label=false)\nplot!(Normal(9.5,0.7*std(x)),label=\"Toxic\")\nplot!(Normal(2.5,std(x)),label=\"No effect\",c=:black)\nplot!([8.5,8.5],[0,0.4],label=\"Measurement\",c=:blue)\n\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"When looking at your results, you intuitively will decide that this is a case of toxic levels. This is done by comparing the distance from the measurement to the apex of each distribution. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,normalize=:probability,label=false)\nplot!(Normal(9.5,0.7*std(x)),label=\"Toxic\",c=:red)\nplot!(Normal(2.5,std(x)),label=\"No effect\",c=:black)\nplot!([8.5,8.5],[0,0.4],label=\"Measurement\",c=:blue)\n\nplot!([8.5,9.5],[0.1675,0.1675],label=\"d1\",c=:red)\n\nplot!([2.5,8.5],[0.1172,0.1172],label=\"d2\",c=:black)\n\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Performing such assessments visually is only possible when we are dealing with very clear-cut cases with limited number of dimensions and categories. When dealing with more complex systems, we need to have a more clear metrics to assign a sample to a specific group of measurements. To generate such metrics we can calculate the posterior probability of the measurement being part of a group, given a prior distribution (i.e. the distribution of each group). We can calculate these posterior probabilities via pdf(-) of ACS.jl package (implemented through package Distributions.jl). So in this case we can perform these calculations as follows:","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nPT = pdf(Normal(9.5,0.7*std(x)),8.5)\nPnE = pdf(Normal(2.5,std(x)),8.5)\nP = [PT,PnE]\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,normalize=:probability,label=false)\nplot!(Normal(9.5,0.7*std(x)),label=\"Toxic\")\nplot!(Normal(2.5,std(x)),label=\"No effect\",c=:black)\nplot!([8.5,8.5],[0,0.4],label=\"Measurment\",c=:blue)\nannotate!(6.5,0.28,string(round(PnE,digits=2)))\nannotate!(10,0.28,string(round(PT,digits=2)))\n\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"These calculations clearly show that the posterior probability of toxic level is larger than the one for the no-effect concentrations resulting in the classification of this new measurement. If dealing with more than one variable, then the product of the posterior probability of each variable will give you the multivariate posterior probability of a measurement being part of a class, given the prior distributions. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B_1n) = prod _i =1^n P(A mid B_i)\n","category":"page"},{"location":"Bayes/#Uncertainty-Assessment","page":"Bayesian Statistics","title":"Uncertainty Assessment","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Let's assume that the figure above is related to the several measurements of the same drug in the wastewater. Our objective here is to estimate what the true estimation of the drug concentration in our samples is. The usual procedure here is to first assume a normal distribution calculating the mean and standard deviation of the data. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,normalize=:probability,label=false)\nplot!(Normal(mean(x),std(x)),label=\"Distribution assuming normality\")\n\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"This is clearly not the best distribution describing our dataset. Another strategy is a nonparametric one i.e. bootstrapping. In this case we try to assess the distribution of mean and the standard deviation of the measurements. For example we can sample 85% of the X over each iteration, enabling the calculation of the mean and the standard deviation of the measurements without the normality assumption. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nfunction r_sample(x,n,prc)\n    m = zeros(n)\n    st = zeros(n)\n\n    for i =1:n \n        tv1 = rand(x,Int(round(prc*length(x))))\n        m[i] = mean(tv1)\n        st[i] = std(tv1)\n\n    end \n\n    return m, st \n    \nend\n\n\nn = 10000\nprc = 0.85\n\nm, st = r_sample(x,n,prc)\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Now if we plot these distributions we can see that the mean is around 5.3 and the standard deviation is around 3.4.","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\np1 = histogram(m,label=\"mean\",normalize=:probability)\nxlabel!(\"Mean\")\nylabel!(\"Probablity\")\n\np2 = histogram(st,label=\"Standard deviation\",normalize=:probability)\nxlabel!(\"Standard deviation\")\nylabel!(\"Probablity\")\n\nplot(p1,p2,layout=(2,1))\n\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"However, these results do not give us a more clear picture of the distribution of the measurements, suggesting that the conventional methods may not be adequate for these assessment. Now let's us Bayesian statistics to tackle this issue. As first step, we can assume a flat/uniform prior distribution. Based on this assumption, our simplified Bayes equation will become: ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"\nP(A mid B) propto P(B mid A) times 1\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"This set up of the Bayes Theorem results in so called maximum likelihood estimate, which is similar to the outcome of the bootstrapping results. However, these results can be improved via incorporation of an informative prior distribution into our calculations. To do so we need to follow the below steps. ","category":"page"},{"location":"Bayes/#Step-1-(prior-selection):","page":"Bayesian Statistics","title":"Step 1 (prior selection):","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"In this case, by looking at the distribution of the data we can consider a gamma distribution as an adequate prior distribution for our data. Gamma distribution similar to normal distribution has two parameters: k shape and theta scale. To estimate these parameters based on our measurements we need to fit this distribution to our data.   ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\npri = fit_mle(Gamma,x)\n\nhistogram(x,bins=5,label=\"Data\",normalize=true)\nplot!(fit_mle(Gamma, x),label=\"Model\")\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probablity\")\n\n","category":"page"},{"location":"Bayes/#Step-2-(assuming-an-average-value):","page":"Bayesian Statistics","title":"Step 2 (assuming an average value):","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"In this step we assume that the true mean of or distribution is around 10 (this is only as example) and the mean actually has a normal distribution with a standard deviation same as X. By doing so, we can calculate P(A mid B) = P(B mid A) times P(B), which is the probability of every measurement given the assumed average. The product of all these probabilities will result in the likelihood of 10 being the true mean of the distribution. Repeating this for all measured values, we will end up with the distribution of mean and its standard deviation. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nfunction uncertainty_estimate(x,pri)\n    target = collect(range(minimum(x),maximum(x),length = 10000))       # Generate a set of potential mean values\n    post = zeros(length(target))                                        # Generate the posterior distribution vector\n\n    for i =1:length(target)\n\n        dist = Normal(target[i],std(x))                                 # Distribution of the assumed mean\n        tv = 1                                                          # Initialize the likelihood values\n        for j = 1:length(x)\n\n            tv = tv * pdf(dist,x[j]) * pdf(pri,x[j])                    # Updating the likelihood over each iteration\n\n        end\n\n        post[i] = tv\n\n\n    end \n\n    post = post ./ sum(post)                                                # Normalize the posterior distribution\n\n    return post, target\nend \n\npost, target = uncertainty_estimate(x,pri)\n\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"The example of assumed mean values would look like this. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,label=\"Data\",normalize=:probability)\nplot!(fit_mle(Gamma,x),label=\"Prior distribution\")\nplot!(Normal(10.5,std(x)),label=\"Potential distribution I\")\nplot!(Normal(8.5,std(x)),label=\"Potential distribution II\")\nplot!(Normal(3.5,std(x)),label=\"Potential distribution III\")\n#plot!(fit_mle(Normal, m),label=\"Model\")\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probability\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"The final distribution on the other hand looks as below. Please note the resolution of this evaluation is highly dependent on the dataset. For example here we are generating a vector of 10000 members between 0 and 10, which may or may not be too fine. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"using ACS \n\nhistogram(x,bins=5,label=\"Data\",normalize=:probability)\nplot!(fit_mle(Gamma,x),label=\"Prior distribution\")\nplot!(fit_mle(Normal, x),label=\"MLE\")\nplot!(target,10^3 .* post,label=\"Posterior distribution * 1000\",c=:black)\n#plot!(fit_mle(Normal, m),label=\"Model\")\nxlabel!(\"Concentration (ppb)\")\nylabel!(\"Probablity\")\n","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"As expected the predicted average based on the posterior probability distribution is very close to the one based on MLE. However, if we look at the uncertainty levels of this distribution (i.e. the sigma), we can see a much more accurate estimation of the distribution, which can be very important in different applications. ","category":"page"},{"location":"Bayes/#Bayesian-regression","page":"Bayesian Statistics","title":"Bayesian regression","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"Bayesian statistics can also be used for solving regression problems. The Bayesian regression is an extension of the the uncertainty assessment, where the Bayes theorem is used to estimate the coefficients of a model starting from a flat prior for all the coefficients. The added step here is to use the sum square errors to improve the prior distribution of the coefficients. Typically these complex modeling problems are solved using a combination of Bayes theorem and Monte Carlo simulations ","category":"page"},{"location":"Bayes/#Additional-Resources","page":"Bayesian Statistics","title":"Additional Resources","text":"","category":"section"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"You can find a lot of resources related to Bayesian Statistics and inference (book), julia package, and python package. Additionally, the following videos are highly informative for better learning the Bayesian statistical concepts. ","category":"page"},{"location":"Bayes/","page":"Bayesian Statistics","title":"Bayesian Statistics","text":"MIT Open Courseware\nDatacamp ","category":"page"}]
}
