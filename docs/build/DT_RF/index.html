<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Decision trees · ACS.jl</title><meta name="title" content="Decision trees · ACS.jl"/><meta property="og:title" content="Decision trees · ACS.jl"/><meta property="twitter:title" content="Decision trees · ACS.jl"/><meta name="description" content="Documentation for ACS.jl."/><meta property="og:description" content="Documentation for ACS.jl."/><meta property="twitter:description" content="Documentation for ACS.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ACS.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../prep/">Preparation</a></li><li><a class="tocitem" href="../svd/">SVD</a></li><li><a class="tocitem" href="../HCA/">HCA</a></li><li><a class="tocitem" href="../KMeans/">k-means</a></li><li class="is-active"><a class="tocitem" href>Decision trees</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#How-to-decision-tree?"><span>How to decision tree?</span></a></li><li><a class="tocitem" href="#Practical-Example"><span>Practical Example</span></a></li><li><a class="tocitem" href="#Random-Forest"><span>Random Forest</span></a></li><li><a class="tocitem" href="#Packages"><span>Packages</span></a></li><li><a class="tocitem" href="#Additional-Resources"><span>Additional Resources</span></a></li></ul></li><li><a class="tocitem" href="../Bayes/">Bayesian Statistics</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Decision trees</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Decision trees</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/EMCMS/ACS.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/EMCMS/ACS.jl/blob/main/docs/src/DT_RF.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Decision-trees-and-random-forrest"><a class="docs-heading-anchor" href="#Decision-trees-and-random-forrest">Decision trees and random forrest</a><a id="Decision-trees-and-random-forrest-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-trees-and-random-forrest" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p><a href="https://en.wikipedia.org/wiki/Decision_tree"><strong>Decision trees</strong></a> are a modeling strategy for dealing with complex data that may include <a href="https://en.wikipedia.org/wiki/Nonlinear_system">non-linearity</a> and/or non-continuity. <strong>Decision trees</strong> can be used for solving both regression and classification problems. <strong>Decision trees</strong> are easy to interpret as you can follow the data within the tree and are able to include the information from different parameters into the final model prediction. The two main disadvantages of <strong>decision trees</strong> are that they tend to have low accuracy and biased towards categorical variables with more levels. These shortcomings of <strong>decision trees</strong> are mitigated by the use of <a href="https://en.wikipedia.org/wiki/Random_forest"><strong>random forest</strong></a>, which is an <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble</a> approach combining the prediction of multiple <strong>decision trees</strong> to generate the final prediction the model. Forest based models are generally considered <a href="https://en.wikipedia.org/wiki/Black_box">black box</a> approaches as they could quickly get very complicated to interpret. However, they are capable of producing reasonable prediction with lower probability of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. </p><h2 id="How-to-decision-tree?"><a class="docs-heading-anchor" href="#How-to-decision-tree?">How to decision tree?</a><a id="How-to-decision-tree?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-decision-tree?" title="Permalink"></a></h2><p>In <strong>decision trees</strong> there are three essential entities: root node, inner node, and leaf node. The root node is a splitting point in a one of the variables where the maximum information gain is obtained. In other word, the highest level of separation is usually obtained at the root node. The inner nodes or nodes are the splitting points for each variable, where over each iteration the purity of the data increases. Finally the leaf node or leaf is the node where further splitting will not result in accuracy gain. Usually the mean of the leaf is considered the model prediction for that specific node. The main formula used for finding a splitting point is the <a href="https://en.wikipedia.org/wiki/Residual_sum_of_squares">residual sum squares (RSS)</a>. Here we are looking for the minimum value of RSS to be used as the splitting point.    </p><p class="math-container">\[
RSS = \sum ^{n}_{i=1} (y_{i} - f(x_{i}))^2 
\]</p><p>In this equation <span>$f(x_{i})$</span> represents the mean of the data points are grouped together while the <span>$y_{i}$</span> is the each individual measurement. By calculating the <em>RSS</em> we are evaluating whether the mean of grouped data points is a good enough estimation of the data. Once one set of data points are grouped, then we use their mean (average) to represent them. This process is repeated for all the variables and data points until every measurement is a relatively pure leaf. A pure leaf is a leaf where the mean of the grouped data points is an accurate estimation of most data points. Let&#39;s see how this works. </p><h2 id="Practical-Example"><a class="docs-heading-anchor" href="#Practical-Example">Practical Example</a><a id="Practical-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Example" title="Permalink"></a></h2><h3 id="Univariate-Case"><a class="docs-heading-anchor" href="#Univariate-Case">Univariate Case</a><a id="Univariate-Case-1"></a><a class="docs-heading-anchor-permalink" href="#Univariate-Case" title="Permalink"></a></h3><p>To start we will use a two dimensional data set, where we would like to predict the relative intensity of our signal based on the injected mass of the calibrant into our mass spectrometer. </p><pre><code class="language-julia hljs">using ACS

file_name = &quot;Rel_res.csv&quot;               # This file is available at: https://github.com/EMCMS/ACS.jl/tree/main/datasets

data = read_ACS_data(file_name)         # Importing the data using an ACS function

scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=false)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="02201c74.svg" alt="Example block output"/><p>As you can see, our measurements are not linear and/or continuous. Thus a simple linear model will not be an adequate estimator of this data. Also our data appears to consist of five clusters for each concentration range (i.e. independent variable). Let&#39;s try to model this data using a <strong>decision tree</strong>.</p><pre><code class="language-julia hljs">using ACS

scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=&quot;data&quot;)
plot!([0,90],[0,100],label=&quot;Linear model&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="3f50dfe9.svg" alt="Example block output"/><p>The first step to build a <strong>decision tree</strong> model is to find the root node (i.e. the first splitting point). Given that we are looking at only one dependent variable (i.e. the relative intensity), we will need to find the first splitting point only in this variable. We will look at the bi-variate case later on. To find the first splitting point, we start with injected masses larger than 2 (i.e. all our measurements except the first one). This implies that we have two groups where the injected mass is either smaller than 2 or larger than two. </p><pre><code class="language-julia hljs">using ACS

scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=&quot;data&quot;)
plot!([0,2],[6,6],label=&quot;&lt; 2&quot;)
plot!([2,90],[mean(data[2:end,&quot;Signal &quot;]),mean(data[2:end,&quot;Signal &quot;])],label=&quot;&gt; 2&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="4a280587.svg" alt="Example block output"/><p>Here we can visually see that the group &quot;&lt; 2&quot; is providing very accurate prediction (<em>RSS</em> = 0) while the second group is not able to predict the instrument response properly. To quantify this, we will use the <em>RSS</em> value for each mass of calibrant being the splitting point. It should be that for the first iteration of the splitting, we will use the point itself as we cannot calculate the mean of a single number.</p><pre><code class="language-julia hljs">using ACS

RSS_l = data[1,&quot;Signal &quot;] - data[1,&quot;Signal &quot;] #

RSS_r = sum((data[2:end,&quot;Signal &quot;] .- mean(data[2:end,&quot;Signal &quot;])).^2)

RSS = RSS_l + RSS_r</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">42160.000000000015</code></pre><p>This process is repeated for every single breaking points. For example let&#39;s repeat these calculations for a mass of 44.3 Fg (i.e. 26th point).</p><pre><code class="language-julia hljs">using ACS

split = 26

RSS_l = sum((data[1:split,&quot;Signal &quot;] .- mean(data[1:split,&quot;Signal &quot;])).^2)

RSS_r = sum((data[split:end,&quot;Signal &quot;] .- mean(data[split:end,&quot;Signal &quot;])).^2)

RSS = RSS_l + RSS_r</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">40315.20615384615</code></pre><pre><code class="language-julia hljs">using ACS

scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=&quot;data&quot;)
plot!([0,data[!,&quot;Conc&quot;][split]],[mean(data[1:split,&quot;Signal &quot;]),mean(data[1:split,&quot;Signal &quot;])],label=&quot;&lt; 44.3&quot;)
plot!([data[!,&quot;Conc&quot;][split],90],[mean(data[split:end,&quot;Signal &quot;]),mean(data[split:end,&quot;Signal &quot;])],label=&quot;&gt; 44.3&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="a7c6cbf4.svg" alt="Example block output"/><p>We can use the below for loop to calculate the <em>RSS</em> for all potential splitting points. </p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Please note that this code assumes that your data is sorted and thus it does not take into  account the actual injected masses (i.e. the independent variable). If you want a more generic solution, you need to take the independent variables into account when creating right and left sides!</p></div></div><pre><code class="language-julia hljs">using ACS


RSS = zeros(length(data[!,&quot;Signal &quot;]))
rss_l = zeros(length(data[!,&quot;Signal &quot;]))
rss_r = zeros(length(data[!,&quot;Signal &quot;]))

for i=1:length(data[!,&quot;Signal &quot;])
    rss_l[i] = sum((data[1:i,&quot;Signal &quot;] .- mean(data[1:i,&quot;Signal &quot;])).^2)

    rss_r[i] = sum((data[i:end,&quot;Signal &quot;] .- mean(data[i:end,&quot;Signal &quot;])).^2)

    RSS[i] = rss_l[i] + rss_r[i]
end


scatter(data[!,&quot;Conc&quot;],RSS, label=&quot;RSS&quot;,markersize = 5)
scatter!(data[!,&quot;Conc&quot;],rss_l, label=&quot;RSS left&quot;, markersize = 2)
scatter!(data[!,&quot;Conc&quot;],rss_r, label=&quot;RSS right&quot;, markersize = 2)
xlabel!(&quot;Mass of calibrant Fg&quot;)
ylabel!(&quot;RSS&quot;)</code></pre><img src="f4ee182d.svg" alt="Example block output"/><p>In the above plot we are able to clearly see both the splitting point and the contribution of each side (i.e. the left vs right RSS). For example for all the points with injection mass of &lt; 17, the model prediction has little to no error in it whereas the model is not accurate at all for injection masses &gt; 17. At the moment, our model has only one splitting point.</p><pre><code class="language-julia hljs">using ACS

split = 10

scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=&quot;data&quot;)
plot!([0,data[!,&quot;Conc&quot;][split]],[mean(data[1:split,&quot;Signal &quot;]),mean(data[1:split,&quot;Signal &quot;])],label=&quot;&lt; 17&quot;)
plot!([data[!,&quot;Conc&quot;][split],90],[mean(data[split:end,&quot;Signal &quot;]),mean(data[split:end,&quot;Signal &quot;])],label=&quot;&gt; 17&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="8463b266.svg" alt="Example block output"/><p>After the first splitting, we need to decide whether each side require further splitting. If we look at the left side the value of 6 (i.e. the model prediction) is fairly accurate prediction. However, we can further split these measurements to get even more accurate estimations. This added accuracy also comes with a high potential for overfitting. There are several methods to assess the presence of overfitting in our model. These approaches include setting a depth limit, setting a minimum number of points to a leaf, <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">information gain</a>, and <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini impurity</a>. Each of these approaches have their own advantages and disadvantages and you should decide the most adequate one, based on your knowledge of the data. </p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>The most intuitive approach to avoid overfitting in tree based models is to set a minimum number of points in terminal leafs. This implies that if there are less than the set value points in a leaf that leaf is considered terminal and will not be considered for further splitting. This number (i.e. the minimum points per leaf) can be estimated by looking at the distribution of your data. Typically, the highest levels of accuracy and robustness is achieved by using a number between 5 and 20 points per leaf. </p></div></div><p>Now that the root of our <strong>decision tree</strong> is identified and we have decided that the left leaf is not going to be further split into more leafs, we need to improve our model for the right side of the splitting point. To do so, we can exclude the left leaf from our data and repeat the <em>RSS</em> calculations for the rest of the data. </p><pre><code class="language-julia hljs">using ACS

conc_1 = data[11:end,&quot;Conc&quot;]
sig_1 = data[11:end,&quot;Signal &quot;]

RSS1 = zeros(length(conc_1))
rss_l1 = zeros(length(conc_1))
rss_r1 = zeros(length(conc_1))

for i=1:length(conc_1)
    rss_l1[i] = sum((sig_1[1:i] .- mean(sig_1[1:i])).^2)

    rss_r1[i] = sum((sig_1[i:end] .- mean(sig_1[i:end])).^2)

    RSS1[i] = rss_l1[i] + rss_r1[i]
end


scatter(data[11:end,&quot;Conc&quot;],RSS1, label=&quot;RSS&quot;,markersize = 5)
scatter!(data[11:end,&quot;Conc&quot;],rss_l1, label=&quot;RSS left&quot;, markersize = 2)
scatter!(data[11:end,&quot;Conc&quot;],rss_r1, label=&quot;RSS right&quot;, markersize = 2)
xlabel!(&quot;Mass of calibrant Fg&quot;)
ylabel!(&quot;RSS&quot;)</code></pre><img src="9960c17b.svg" alt="Example block output"/><p>As you can clearly see from the above plot the measurements larger than 68 Fg are grouped together and since the number of points in that leaf is smaller or equal to the set limit of 10 points, thus this is our second leaf. Currently our model has three potential predictions depending on the provided mass of the injected calibrant. For the two edge leafs, our model does very well in terms of accuracy while for the center leaf further splitting is necessary. We can perform this by repeating the above steps.  </p><pre><code class="language-julia hljs">using ACS

split1 = 10
split2 = 40


scatter(data[!,&quot;Conc&quot;],data[!,&quot;Signal &quot;], label=&quot;data&quot;)
plot!([0,data[!,&quot;Conc&quot;][split1]],[mean(data[1:split1,&quot;Signal &quot;]),mean(data[1:split1,&quot;Signal &quot;])],label=&quot;&lt; 17&quot;)

plot!([data[!,&quot;Conc&quot;][split1],data[!,&quot;Conc&quot;][split2]],[mean(data[split1:split2,&quot;Signal &quot;]),mean(data[split1:split2,&quot;Signal &quot;])],label=&quot;17 &lt; x  &lt; 68&quot;)

plot!([data[!,&quot;Conc&quot;][split2],90],[mean(data[split2:end,&quot;Signal &quot;]),mean(data[split2:end,&quot;Signal &quot;])],label=&quot;&gt; 68&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="7fdc48af.svg" alt="Example block output"/><h3 id="Multivariate-Case"><a class="docs-heading-anchor" href="#Multivariate-Case">Multivariate Case</a><a id="Multivariate-Case-1"></a><a class="docs-heading-anchor-permalink" href="#Multivariate-Case" title="Permalink"></a></h3><p>So far we have only looked at a univariate case. For a system with multiple variables, the process is the same as the univariate one and it is performed over each variable separately and then the minimum <em>RSS</em> value by the splitting points are compared, the variable with the smallest <em>RSS</em> value will take precedent. For example in case of our dataset, now we can use both variables in our model. </p><pre><code class="language-julia hljs">using ACS

scatter(data[!,&quot;Conc&quot;][data[!,&quot;Mode&quot;] .== 1],data[!,&quot;Signal &quot;][data[!,&quot;Mode&quot;] .== 1], label=&quot;Positive mode&quot;)
scatter!(data[!,&quot;Conc&quot;][data[!,&quot;Mode&quot;] .== -1],data[!,&quot;Signal &quot;][data[!,&quot;Mode&quot;] .== -1], label=&quot;Negative mode&quot;)
xlabel!(&quot;Mass calibrant (Fg)&quot;)
ylabel!(&quot;Relative intensity (%)&quot;)</code></pre><img src="7f9283d5.svg" alt="Example block output"/><p>In this case, the mass of the calibrant will still be the root node of our tree while the second node will be related to the acquisition mode.</p><h2 id="Random-Forest"><a class="docs-heading-anchor" href="#Random-Forest">Random Forest</a><a id="Random-Forest-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forest" title="Permalink"></a></h2><p>As it was mentioned above <strong>random forest</strong> was introduced as a means for overcoming the issues associated with the <strong>decision trees</strong> namely the lack of accuracy and robustness. The basic concept behind <strong>random forest</strong> is the use of multiple trees and the resampling strategies such as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a>. Here we first generate a large number of trees (e.g. hundreds) and with each of those trees we model a bootstrap sample of the original data. This implies that no two trees have the same dataset to model and our <strong>random forest</strong> model has as many prediction as the number of trees. At this point the <strong>random forest</strong> model tallies the distribution of the predictions and outputs the prediction with the highest likelihood (i.e. the most frequent prediction). This strategy, usually, results in a much more robust and accurate models, mitigating the limitations of <strong>decision trees</strong>. When building <strong>random forest</strong> models, one can decide how to distribute the data across the trees. For example, one of the most communly used approaches to build bootstrapping samples is <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a>, where not all the variables and the measurements are given to each tree. When performing <strong>random forest</strong> modeling, independently from the used package, there are three <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameters</a> that must be optimized namely: the number of trees, the minimum points per leaf, and bootstrapping conditions. Each of these parameters have their own criterion for being optimized and thus they need to be considered all together. For example, we want to keep the number of trees to the minimum while having highest possible accuracy of the <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross validation</a> and <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets">test set</a>. One way to deal with this is to perform <a href="https://scikit-learn.org/stable/modules/grid_search.html">grid search</a>, which could be computationally expensive, depending on the resolution needed. </p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>For the number of trees usually you are looking for the optimized number between 100 and 500 trees. Please note this is highly case dependent and may not be correct for your specific dataset. For the bootstrapping parameter, in case of classification a square root of the number of variables and for regression 1/3 of the number of variable can be used as starting points. </p></div></div><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>When building any type of model, you need to make sure that the variables are <a href="https://en.wikipedia.org/wiki/Feature_scaling">scaled</a>. Otherwise, the variable with the largest magnitude will dominate your model, even though, it may not be the most relevant variable to the model.</p></div></div><h2 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h2><p>The package <a href="https://github.com/cstjean/ScikitLearn.jl">ScikitLearn.jl</a> which is a julia wrapper of the <a href="https://scikit-learn.org/stable/index.html">python package</a> provides access to a wide variety of <strong>random forest</strong> implementations. Below is an example of the usage case for a classification problem.</p><pre><code class="language-julia hljs">using ACS

@sk_import ensemble: RandomForestClassifier

data = dataset(&quot;datasets&quot;, &quot;iris&quot;)

y = data[!,&quot;Species&quot;]
Y = zeros(length(y))
Y[y .== &quot;setosa&quot;] .= 1
Y[y .== &quot;versicolor&quot;] .= 2
X = Matrix(data[:,1:4]); # The first four columns are selected for this

clf = RandomForestClassifier(n_estimators=200, min_samples_leaf=5,
oob_score =true, max_features= &quot;auto&quot;,n_jobs=-1).fit(X,Y)
accuracy_cl = clf.score(X,Y)
</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Please note that the above implementation is not following the best practices for this type of modeling as the data is not split into training set and test set. Also, no cross-validation is performed. Finally none of the hyperparameters here have been optimized.  </p></div></div><h2 id="Additional-Resources"><a class="docs-heading-anchor" href="#Additional-Resources">Additional Resources</a><a id="Additional-Resources-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Resources" title="Permalink"></a></h2><p>There are several resources (including videos on YouTube) for better understanding how <strong>decision trees</strong> and <strong>random forests</strong> work. The documentation of <a href="https://scikit-learn.org/stable/index.html">SKLearn package</a> is one of the best resources for this. Additionally, you can follow this lecture by <a href="https://www.youtube.com/watch?v=4EOCQJgqAOY">Kilian Weinberger</a> from Cornell. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../KMeans/">« k-means</a><a class="docs-footer-nextpage" href="../Bayes/">Bayesian Statistics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 25 February 2025 22:04">Tuesday 25 February 2025</span>. Using Julia version 1.10.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
